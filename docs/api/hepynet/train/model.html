<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>hepynet.train.model API documentation</title>
<meta name="description" content="Model wrapper class for DNN training" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>hepynet.train.model</code></h1>
</header>
<section id="section-intro">
<p>Model wrapper class for DNN training</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;Model wrapper class for DNN training&#34;&#34;&#34;
import copy
import datetime
import glob
import logging
import math
import pathlib
import typing

logger = logging.getLogger(&#34;hepynet&#34;)
import keras
import numpy as np
import pandas as pd
import tensorflow as tf
import yaml

# fix tensorflow 2.2 issue
gpus = tf.config.experimental.list_physical_devices(&#34;GPU&#34;)
if gpus:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices(&#34;GPU&#34;)
        logger.info(
            f&#34;GPU availability: {len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs&#34;
        )
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        logger.error(e)
from keras import backend as K
from keras.callbacks import ModelCheckpoint, TensorBoard, callbacks
from keras.layers import Dense, Dropout
from keras.models import Sequential
from keras.optimizers import SGD, Adagrad, Adam, RMSprop

from hepynet.common import array_utils, common_utils
from hepynet.data_io import feed_box
from hepynet.train import evaluate, train_utils


# self-defined metrics functions
def plain_acc(y_true, y_pred):
    return K.mean(K.less(K.abs(y_pred * 1.0 - y_true * 1.0), 0.5))
    # return 1-K.mean(K.abs(y_pred-y_true))


def get_model_class(model_class: str):
    if model_class == &#34;Model_Sequential_Flat&#34;:
        return Model_Sequential_Flat
    else:
        logger.critical(f&#34;Unsupported model class: {model_class}&#34;)
        exit(1)


class Model_Base(object):
    &#34;&#34;&#34;Base model of deep neural network
    &#34;&#34;&#34;

    def __init__(self, name):
        &#34;&#34;&#34;Initialize model.

        Args:
        name: str
            Name of the model.

        &#34;&#34;&#34;
        self._model_create_time = str(datetime.datetime.now())
        self._model_is_compiled = False
        self._model_is_loaded = False
        self._model_is_saved = False
        self._model_is_trained = False
        self._model_name = name
        self._model_save_path = None
        self._train_history = None


class Model_Sequential_Base(Model_Base):
    &#34;&#34;&#34;Sequential model base.

    Note:
        This class should not be used directly
    &#34;&#34;&#34;

    def __init__(
        self, job_config,
    ):
        &#34;&#34;&#34;Initialize model.&#34;&#34;&#34;
        self._job_config = job_config.clone()
        tc = self._job_config.train
        ic = self._job_config.input
        super().__init__(tc.model_name)
        # Model parameters
        self._model_label = &#34;mod_seq_base&#34;
        self._model_note = &#34;Basic sequential model.&#34;
        self._model_input_dim = len(ic.selected_features)
        self._model = Sequential()
        # Arrays
        self._array_prepared = False
        self._selected_features = ic.selected_features
        self._model_meta = {
            &#34;norm_dict&#34;: None,
        }
        # Report
        self._save_tb_logs = tc.save_tb_logs
        self._tb_logs_path = tc.tb_logs_path

    def get_model(self):
        &#34;&#34;&#34;Returns model.&#34;&#34;&#34;
        if not self._model_is_compiled:
            logger.warning(&#34;Model is not compiled&#34;)
        return self._model

    def get_model_meta(self):
        return self._model_meta

    def get_train_history(self):
        &#34;&#34;&#34;Returns train history.&#34;&#34;&#34;
        if not self._model_is_compiled:
            logger.warning(&#34;Model is not compiled&#34;)
        if self._train_history is None:
            logger.warning(&#34;Empty training history found&#34;)
        return self._train_history

    def get_train_performance_meta(self):
        &#34;&#34;&#34;Returns meta data of training performance

        Note:
            This function should be called after show_performance and
        plot_significance_scan being called, otherwise &#34;-&#34; will be used as
        content.

        &#34;&#34;&#34;
        performance_meta_dict = {}
        # try collect significance scan result
        try:
            performance_meta_dict[&#34;original_significance&#34;] = self.original_significance
            performance_meta_dict[&#34;max_significance&#34;] = self.max_significance
            performance_meta_dict[
                &#34;max_significance_threshold&#34;
            ] = self.max_significance_threshold
        except:
            performance_meta_dict[&#34;original_significance&#34;] = &#34;-&#34;
            performance_meta_dict[&#34;max_significance&#34;] = &#34;-&#34;
            performance_meta_dict[&#34;max_significance_threshold&#34;] = &#34;-&#34;
        # try collect auc value
        try:
            # performance_meta_dict[&#34;auc_train&#34;] = self.auc_train
            # performance_meta_dict[&#34;auc_test&#34;] = self.auc_test
            # performance_meta_dict[&#34;auc_train_original&#34;] = self.auc_train_original
            # performance_meta_dict[&#34;auc_test_original&#34;] = self.auc_test_original
            pass
        except:
            # performance_meta_dict[&#34;auc_train&#34;] = &#34;-&#34;
            # performance_meta_dict[&#34;auc_test&#34;] = &#34;-&#34;
            # performance_meta_dict[&#34;auc_train_original&#34;] = &#34;-&#34;
            # performance_meta_dict[&#34;auc_test_original&#34;] = &#34;-&#34;
            pass
        return performance_meta_dict

    def get_corrcoef(self) -&gt; dict:
        bkg_array, _ = self.feedbox.get_reweight(
            &#34;xb&#34;, array_key=&#34;all&#34;, reset_mass=False
        )
        d_bkg = pd.DataFrame(data=bkg_array, columns=list(self._selected_features),)
        bkg_matrix = d_bkg.corr()
        sig_array, _ = self.feedbox.get_reweight(
            &#34;xs&#34;, array_key=&#34;all&#34;, reset_mass=False
        )
        d_sig = pd.DataFrame(data=sig_array, columns=list(self._selected_features),)
        sig_matrix = d_sig.corr()
        corrcoef_matrix_dict = {}
        corrcoef_matrix_dict[&#34;bkg&#34;] = bkg_matrix
        corrcoef_matrix_dict[&#34;sig&#34;] = sig_matrix
        return corrcoef_matrix_dict

    def load_model(self, load_dir, _model_name, job_name=&#34;*&#34;, date=&#34;*&#34;, version=&#34;*&#34;):
        &#34;&#34;&#34;Loads saved model.&#34;&#34;&#34;
        # Search possible files
        search_pattern = (
            load_dir + &#34;/&#34; + date + &#34;_&#34; + job_name + &#34;_&#34; + version + &#34;/models&#34;
        )
        model_dir_list = glob.glob(search_pattern)
        if not model_dir_list:
            search_pattern = &#34;/work/&#34; + search_pattern
            logger.debug(f&#34;search pattern:{search_pattern}&#34;)
            model_dir_list = glob.glob(search_pattern)
        model_dir_list = sorted(model_dir_list)
        # Choose the newest one
        if len(model_dir_list) &lt; 1:
            raise FileNotFoundError(&#34;Model file that matched the pattern not found.&#34;)
        model_dir = model_dir_list[-1]
        if len(model_dir_list) &gt; 1:
            logger.info(
                &#34;More than one valid model file found, maybe you should try to specify more infomation.&#34;
            )
            logger.info(f&#34;Loading the last matched model path: {model_dir}&#34;)
        else:
            logger.info(f&#34;Loading model at: {model_dir}&#34;)
        self._model = keras.models.load_model(
            model_dir + &#34;/&#34; + _model_name + &#34;.h5&#34;,
            custom_objects={&#34;plain_acc&#34;: plain_acc},
        )  # it&#39;s important to specify
        # custom objects
        self._model_is_loaded = True
        # Load parameters
        # try:
        paras_path = model_dir + &#34;/&#34; + _model_name + &#34;_paras.yaml&#34;
        self.load_model_parameters(paras_path)
        self.model_paras_is_loaded = True
        # except:
        #    logger.warning(&#34;Model parameters not successfully loaded.&#34;)
        logger.info(&#34;Model loaded.&#34;)

    def load_model_with_path(self, model_path, paras_path=None):
        &#34;&#34;&#34;Loads model with given path

        Note:
            Should load model parameters manually.
        &#34;&#34;&#34;
        self._model = keras.models.load_model(
            model_path, custom_objects={&#34;plain_acc&#34;: plain_acc},
        )  # it&#39;s important to specify
        if paras_path is not None:
            try:
                self.load_model_parameters(paras_path)
                self.model_paras_is_loaded = True
            except:
                logger.warning(&#34;Model parameters not successfully loaded.&#34;)
        logger.info(&#34;Model loaded.&#34;)

    def load_model_parameters(self, paras_path):
        &#34;&#34;&#34;Retrieves model parameters from yaml file.&#34;&#34;&#34;
        with open(paras_path, &#34;r&#34;) as paras_file:
            paras_dict = yaml.load(paras_file, Loader=yaml.UnsafeLoader)
        # sorted by alphabet
        self._job_config.update(paras_dict[&#34;job_config_dict&#34;])

        model_meta_save = paras_dict[&#34;model_meta&#34;]
        self._model_meta = model_meta_save
        self._model_name = model_meta_save[&#34;model_name&#34;]
        self._model_label = model_meta_save[&#34;model_label&#34;]
        self._model_note = model_meta_save[&#34;model_note&#34;]
        self._model_create_time = model_meta_save[&#34;model_create_time&#34;]
        self._model_is_compiled = model_meta_save[&#34;model_is_compiled&#34;]
        self._model_is_saved = model_meta_save[&#34;model_is_saved&#34;]
        self._model_is_trained = model_meta_save[&#34;model_is_trained&#34;]

        self._train_history = paras_dict[&#34;train_history&#34;]

    def save_model(self, save_dir=None, file_name=None):
        &#34;&#34;&#34;Saves trained model.

        Args:
            save_dir: str
            Path to save model.

        &#34;&#34;&#34;
        # Define save path
        if save_dir is None:
            save_dir = &#34;./models&#34;
        if file_name is None:
            datestr = datetime.date.today().strftime(&#34;%Y-%m-%d&#34;)
            file_name = self._model_name + &#34;_&#34; + self._model_label + &#34;_&#34; + datestr
        # Check path
        save_path = save_dir + &#34;/&#34; + file_name + &#34;.h5&#34;
        pathlib.Path(save_dir).mkdir(parents=True, exist_ok=True)
        # Save
        self._model.save(save_path)
        self._model_save_path = save_path
        logger.debug(f&#34;model: {self._model_name} has been saved to: {save_path}&#34;)
        # update path for yaml
        save_path = save_dir + &#34;/&#34; + file_name + &#34;_paras.yaml&#34;
        self.save_model_paras(save_path)
        logger.debug(f&#34;model parameters has been saved to: {save_path}&#34;)
        self._model_is_saved = True

    def save_model_paras(self, save_path):
        &#34;&#34;&#34;Save model parameters to yaml file.&#34;&#34;&#34;
        paras_dict = dict()
        paras_dict[&#34;job_config_dict&#34;] = self._job_config.get_config_dict()

        model_meta_save = copy.deepcopy(self._model_meta)
        model_meta_save[&#34;model_name&#34;] = self._model_name
        model_meta_save[&#34;model_label&#34;] = self._model_label
        model_meta_save[&#34;model_note&#34;] = self._model_note
        model_meta_save[&#34;model_create_time&#34;] = self._model_create_time
        model_meta_save[&#34;model_is_compiled&#34;] = self._model_is_compiled
        model_meta_save[&#34;model_is_saved&#34;] = self._model_is_saved
        model_meta_save[&#34;model_is_trained&#34;] = self._model_is_trained
        paras_dict[&#34;model_meta&#34;] = model_meta_save

        paras_dict[&#34;train_history&#34;] = self._train_history

        with open(save_path, &#34;w&#34;) as write_file:
            yaml.dump(paras_dict, write_file, indent=2)

    def set_inputs(self, job_config) -&gt; None:
        &#34;&#34;&#34;Prepares array for training.&#34;&#34;&#34;
        feedbox = feed_box.Feedbox(
            job_config,
            model_meta=self.get_model_meta(),
        )
        if job_config.job.job_type == &#34;apply&#34;:
            feedbox.load_sig_arrays()
            feedbox.load_bkg_arrays()
        self._model_meta[&#34;norm_dict&#34;] = copy.deepcopy(feedbox.get_norm_dict())
        self.feedbox = feedbox
        self._array_prepared = feedbox._array_prepared

    def train(
        self, job_config, model_save_dir=None, file_name=None,
    ):
        &#34;&#34;&#34;Performs training.&#34;&#34;&#34;

        # prepare config alias
        ic = self._job_config.input
        tc = self._job_config.train

        # Check
        if self._model_is_compiled == False:
            logging.critical(&#34;DNN model is not yet compiled&#34;)
            exit(1)
        if self._array_prepared == False:
            logging.critical(&#34;Training data is not ready.&#34;)
            exit(1)

        # Train
        logger.info(&#34;-&#34; * 40)
        logger.info(f&#34;Training start. Using model: {self._model_name}&#34;)
        logger.info(f&#34;Model info: {self._model_note}&#34;)
        self.class_weight = {1: tc.sig_class_weight, 0: tc.bkg_class_weight}
        ## setup callbacks
        train_callbacks = []
        if self._save_tb_logs:  # TODO: add back this function
            pass
            # if self._tb_logs_path is None:
            #    self._tb_logs_path = &#34;temp_logs/{}&#34;.format(self._model_label)
            #    logger.warning(
            #        &#34;TensorBoard logs path not specified, set path to: {}&#34;.format(
            #            self._tb_logs_path
            #        )
            #    )
            # tb_callback = TensorBoard(log_dir=self._tb_logs_path, histogram_freq=1)
            # train_callbacks.append(tb_callback)
        if tc.use_early_stop:
            early_stop_callback = callbacks.EarlyStopping(
                monitor=tc.early_stop_paras.monitor,
                min_delta=tc.early_stop_paras.min_delta,
                patience=tc.early_stop_paras.patience,
                mode=tc.early_stop_paras.mode,
                restore_best_weights=tc.early_stop_paras.restore_best_weights,
            )
            train_callbacks.append(early_stop_callback)
        ## set up check point to save model in each epoch
        if model_save_dir is None:
            model_save_dir = &#34;./models&#34;
        pathlib.Path(model_save_dir).mkdir(parents=True, exist_ok=True)
        if file_name is None:
            file_name = self._model_name
        path_pattern = model_save_dir + &#34;/&#34; + file_name + &#34;_epoch{epoch:03d}.h5&#34;
        checkpoint = ModelCheckpoint(path_pattern, monitor=&#34;val_loss&#34;)
        train_callbacks.append(checkpoint)
        ## check input
        train_test_dict = self.feedbox.get_train_test_arrays(
            sig_key=ic.sig_key,
            bkg_key=ic.bkg_key,
            multi_class_bkgs=tc.output_bkg_node_names,
            output_keys=train_utils.COMB_KEYS,
        )
        self.feedbox = None
        x_train = train_test_dict[&#34;x_train&#34;]
        x_test = train_test_dict[&#34;x_test&#34;]
        y_train = train_test_dict[&#34;y_train&#34;]
        y_test = train_test_dict[&#34;y_test&#34;]
        wt_train = train_test_dict[&#34;wt_train&#34;]
        wt_test = train_test_dict[&#34;wt_test&#34;]
        train_test_dict = None
        self.get_model().summary()
        ## train
        history_obj = self.get_model().fit(
            x_train,
            y_train,
            batch_size=tc.batch_size,
            epochs=tc.epochs,
            validation_split=tc.val_split,
            sample_weight=wt_train,
            callbacks=train_callbacks,
            verbose=tc.verbose,
        )
        self._train_history = history_obj.history
        logger.info(&#34;Training finished.&#34;)

        # Evaluation
        logger.info(&#34;Evaluate with test dataset:&#34;)
        score = self.get_model().evaluate(
            x_test, y_test, verbose=tc.verbose, sample_weight=wt_test,
        )

        if not isinstance(score, typing.Iterable):
            logger.info(f&#34;&gt; test loss: {score}&#34;)
        else:
            for i, metric in enumerate(self.get_model().metrics_names):
                logger.info(f&#34;&gt; test - {metric}: {score[i]}&#34;)

        # update status
        self._model_is_trained = True

    &#39;&#39;&#39;
    def tuning_train(
        self,
        sig_key=&#34;all&#34;,
        bkg_key=&#34;all&#34;,
        batch_size=128,
        epochs=20,
        val_split=0.25,
        sig_class_weight=1.0,
        bkg_class_weight=1.0,
        verbose=1,
    ):
        &#34;&#34;&#34;Performs quick training for hyperparameters tuning.&#34;&#34;&#34;
        # Check
        if self._model_is_compiled == False:
            raise ValueError(&#34;DNN model is not yet compiled&#34;)
        if self._array_prepared == False:
            raise ValueError(&#34;Training data is not ready.&#34;)
        # separate validation samples
        train_test_dict = self.feedbox.get_train_test_arrays(
            sig_key=sig_key,
            bkg_key=bkg_key,
            multi_class_bkgs=self.model_hypers[&#34;output_bkg_node_names&#34;],
            use_selected=False,
        )
        x_train = train_test_dict[&#34;x_train&#34;]
        y_train = train_test_dict[&#34;y_train&#34;]
        x_train_selected = train_test_dict[&#34;x_train_selected&#34;]
        num_val = math.ceil(len(y_train) * val_split)
        x_tr = x_train_selected[:-num_val, :]
        x_val = x_train_selected[-num_val:, :]
        y_tr = y_train[:-num_val]
        y_val = y_train[-num_val:]
        wt_tr = x_train[:-num_val, -1]
        wt_val = x_train[-num_val:, -1]
        val_tuple = (x_val, y_val, wt_val)
        self.x_tr = x_tr
        self.x_val = x_val
        self.y_tr = y_tr
        self.y_val = y_val
        self.wt_tr = wt_tr
        self.wt_val = wt_val
        # Train
        self.class_weight = {1: sig_class_weight, 0: bkg_class_weight}
        train_callbacks = []
        if self.model_hypers[&#34;use_early_stop&#34;]:
            early_stop_callback = callbacks.EarlyStopping(
                monitor=self.model_hypers[&#34;early_stop_paras&#34;][&#34;monitor&#34;],
                min_delta=self.model_hypers[&#34;early_stop_paras&#34;][&#34;min_delta&#34;],
                patience=self.model_hypers[&#34;early_stop_paras&#34;][&#34;patience&#34;],
                mode=self.model_hypers[&#34;early_stop_paras&#34;][&#34;mode&#34;],
                restore_best_weights=self.model_hypers[&#34;early_stop_paras&#34;][
                    &#34;restore_best_weights&#34;
                ],
            )
            train_callbacks.append(early_stop_callback)
        self._train_history = self.get_model().fit(
            x_tr,
            y_tr,
            batch_size=batch_size,
            epochs=epochs,
            validation_data=val_tuple,
            class_weight=self.class_weight,
            sample_weight=wt_tr,
            callbacks=train_callbacks,
            verbose=verbose,
        )
        # Final evaluation
        score = self.get_model().evaluate(
            x_val, y_val, verbose=verbose, sample_weight=wt_val
        )
        # update status
        self._model_is_trained = True
        return score[0]
    &#39;&#39;&#39;


class Model_Sequential_Flat(Model_Sequential_Base):
    &#34;&#34;&#34;Sequential model optimized with old ntuple at Sep. 9th 2019.

    Major modification based on 1002 model:
        1. Change structure to make quantity of nodes decrease with layer num.

    &#34;&#34;&#34;

    def __init__(
        self, job_config,
    ):
        super().__init__(job_config)

        self._model_label = &#34;mod_seq&#34;
        self._model_note = &#34;Sequential model with flexible layers and nodes.&#34;

    def compile(self):
        &#34;&#34;&#34; Compile model, function to be changed in the future.&#34;&#34;&#34;
        tc = self._job_config.train
        # Add layers
        for layer in range(tc.layers):
            # input layer
            if layer == 0:
                self._model.add(
                    Dense(
                        tc.nodes,
                        kernel_initializer=&#34;glorot_uniform&#34;,
                        activation=&#34;relu&#34;,
                        input_dim=self._model_input_dim,
                    )
                )
            # hidden layers
            else:
                self._model.add(
                    Dense(
                        tc.nodes,
                        kernel_initializer=&#34;glorot_uniform&#34;,
                        activation=&#34;relu&#34;,
                    )
                )
            if tc.dropout_rate != 0:
                self._model.add(Dropout(tc.dropout_rate))
        # output layer
        if tc.output_bkg_node_names:
            num_nodes_out = len(tc.output_bkg_node_names) + 1
        else:
            num_nodes_out = 1
        self._model.add(
            Dense(
                num_nodes_out,
                kernel_initializer=&#34;glorot_uniform&#34;,
                activation=&#34;sigmoid&#34;,
            )
        )
        # Compile
        # transfer self-defined metrics into real function
        metrics = copy.deepcopy(tc.train_metrics)
        weighted_metrics = copy.deepcopy(tc.train_metrics_weighted)
        if metrics is not None and &#34;plain_acc&#34; in metrics:
            index = metrics.index(&#34;plain_acc&#34;)
            metrics[index] = plain_acc
        if weighted_metrics is not None and &#34;plain_acc&#34; in weighted_metrics:
            index = weighted_metrics.index(&#34;plain_acc&#34;)
            weighted_metrics[index] = plain_acc
        # compile model
        self._model.compile(
            loss=&#34;binary_crossentropy&#34;,
            optimizer=SGD(
                lr=tc.learn_rate,
                decay=tc.learn_rate_decay,
                momentum=tc.momentum,
                nesterov=tc.nesterov,
            ),
            metrics=metrics,
            weighted_metrics=weighted_metrics,
        )
        self._model_is_compiled = True</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="hepynet.train.model.get_model_class"><code class="name flex">
<span>def <span class="ident">get_model_class</span></span>(<span>model_class: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_model_class(model_class: str):
    if model_class == &#34;Model_Sequential_Flat&#34;:
        return Model_Sequential_Flat
    else:
        logger.critical(f&#34;Unsupported model class: {model_class}&#34;)
        exit(1)</code></pre>
</details>
</dd>
<dt id="hepynet.train.model.plain_acc"><code class="name flex">
<span>def <span class="ident">plain_acc</span></span>(<span>y_true, y_pred)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plain_acc(y_true, y_pred):
    return K.mean(K.less(K.abs(y_pred * 1.0 - y_true * 1.0), 0.5))</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="hepynet.train.model.Model_Base"><code class="flex name class">
<span>class <span class="ident">Model_Base</span></span>
<span>(</span><span>name)</span>
</code></dt>
<dd>
<div class="desc"><p>Base model of deep neural network</p>
<p>Initialize model.</p>
<p>Args:
name: str
Name of the model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Model_Base(object):
    &#34;&#34;&#34;Base model of deep neural network
    &#34;&#34;&#34;

    def __init__(self, name):
        &#34;&#34;&#34;Initialize model.

        Args:
        name: str
            Name of the model.

        &#34;&#34;&#34;
        self._model_create_time = str(datetime.datetime.now())
        self._model_is_compiled = False
        self._model_is_loaded = False
        self._model_is_saved = False
        self._model_is_trained = False
        self._model_name = name
        self._model_save_path = None
        self._train_history = None</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="hepynet.train.model.Model_Sequential_Base" href="#hepynet.train.model.Model_Sequential_Base">Model_Sequential_Base</a></li>
</ul>
</dd>
<dt id="hepynet.train.model.Model_Sequential_Base"><code class="flex name class">
<span>class <span class="ident">Model_Sequential_Base</span></span>
<span>(</span><span>job_config)</span>
</code></dt>
<dd>
<div class="desc"><p>Sequential model base.</p>
<h2 id="note">Note</h2>
<p>This class should not be used directly</p>
<p>Initialize model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Model_Sequential_Base(Model_Base):
    &#34;&#34;&#34;Sequential model base.

    Note:
        This class should not be used directly
    &#34;&#34;&#34;

    def __init__(
        self, job_config,
    ):
        &#34;&#34;&#34;Initialize model.&#34;&#34;&#34;
        self._job_config = job_config.clone()
        tc = self._job_config.train
        ic = self._job_config.input
        super().__init__(tc.model_name)
        # Model parameters
        self._model_label = &#34;mod_seq_base&#34;
        self._model_note = &#34;Basic sequential model.&#34;
        self._model_input_dim = len(ic.selected_features)
        self._model = Sequential()
        # Arrays
        self._array_prepared = False
        self._selected_features = ic.selected_features
        self._model_meta = {
            &#34;norm_dict&#34;: None,
        }
        # Report
        self._save_tb_logs = tc.save_tb_logs
        self._tb_logs_path = tc.tb_logs_path

    def get_model(self):
        &#34;&#34;&#34;Returns model.&#34;&#34;&#34;
        if not self._model_is_compiled:
            logger.warning(&#34;Model is not compiled&#34;)
        return self._model

    def get_model_meta(self):
        return self._model_meta

    def get_train_history(self):
        &#34;&#34;&#34;Returns train history.&#34;&#34;&#34;
        if not self._model_is_compiled:
            logger.warning(&#34;Model is not compiled&#34;)
        if self._train_history is None:
            logger.warning(&#34;Empty training history found&#34;)
        return self._train_history

    def get_train_performance_meta(self):
        &#34;&#34;&#34;Returns meta data of training performance

        Note:
            This function should be called after show_performance and
        plot_significance_scan being called, otherwise &#34;-&#34; will be used as
        content.

        &#34;&#34;&#34;
        performance_meta_dict = {}
        # try collect significance scan result
        try:
            performance_meta_dict[&#34;original_significance&#34;] = self.original_significance
            performance_meta_dict[&#34;max_significance&#34;] = self.max_significance
            performance_meta_dict[
                &#34;max_significance_threshold&#34;
            ] = self.max_significance_threshold
        except:
            performance_meta_dict[&#34;original_significance&#34;] = &#34;-&#34;
            performance_meta_dict[&#34;max_significance&#34;] = &#34;-&#34;
            performance_meta_dict[&#34;max_significance_threshold&#34;] = &#34;-&#34;
        # try collect auc value
        try:
            # performance_meta_dict[&#34;auc_train&#34;] = self.auc_train
            # performance_meta_dict[&#34;auc_test&#34;] = self.auc_test
            # performance_meta_dict[&#34;auc_train_original&#34;] = self.auc_train_original
            # performance_meta_dict[&#34;auc_test_original&#34;] = self.auc_test_original
            pass
        except:
            # performance_meta_dict[&#34;auc_train&#34;] = &#34;-&#34;
            # performance_meta_dict[&#34;auc_test&#34;] = &#34;-&#34;
            # performance_meta_dict[&#34;auc_train_original&#34;] = &#34;-&#34;
            # performance_meta_dict[&#34;auc_test_original&#34;] = &#34;-&#34;
            pass
        return performance_meta_dict

    def get_corrcoef(self) -&gt; dict:
        bkg_array, _ = self.feedbox.get_reweight(
            &#34;xb&#34;, array_key=&#34;all&#34;, reset_mass=False
        )
        d_bkg = pd.DataFrame(data=bkg_array, columns=list(self._selected_features),)
        bkg_matrix = d_bkg.corr()
        sig_array, _ = self.feedbox.get_reweight(
            &#34;xs&#34;, array_key=&#34;all&#34;, reset_mass=False
        )
        d_sig = pd.DataFrame(data=sig_array, columns=list(self._selected_features),)
        sig_matrix = d_sig.corr()
        corrcoef_matrix_dict = {}
        corrcoef_matrix_dict[&#34;bkg&#34;] = bkg_matrix
        corrcoef_matrix_dict[&#34;sig&#34;] = sig_matrix
        return corrcoef_matrix_dict

    def load_model(self, load_dir, _model_name, job_name=&#34;*&#34;, date=&#34;*&#34;, version=&#34;*&#34;):
        &#34;&#34;&#34;Loads saved model.&#34;&#34;&#34;
        # Search possible files
        search_pattern = (
            load_dir + &#34;/&#34; + date + &#34;_&#34; + job_name + &#34;_&#34; + version + &#34;/models&#34;
        )
        model_dir_list = glob.glob(search_pattern)
        if not model_dir_list:
            search_pattern = &#34;/work/&#34; + search_pattern
            logger.debug(f&#34;search pattern:{search_pattern}&#34;)
            model_dir_list = glob.glob(search_pattern)
        model_dir_list = sorted(model_dir_list)
        # Choose the newest one
        if len(model_dir_list) &lt; 1:
            raise FileNotFoundError(&#34;Model file that matched the pattern not found.&#34;)
        model_dir = model_dir_list[-1]
        if len(model_dir_list) &gt; 1:
            logger.info(
                &#34;More than one valid model file found, maybe you should try to specify more infomation.&#34;
            )
            logger.info(f&#34;Loading the last matched model path: {model_dir}&#34;)
        else:
            logger.info(f&#34;Loading model at: {model_dir}&#34;)
        self._model = keras.models.load_model(
            model_dir + &#34;/&#34; + _model_name + &#34;.h5&#34;,
            custom_objects={&#34;plain_acc&#34;: plain_acc},
        )  # it&#39;s important to specify
        # custom objects
        self._model_is_loaded = True
        # Load parameters
        # try:
        paras_path = model_dir + &#34;/&#34; + _model_name + &#34;_paras.yaml&#34;
        self.load_model_parameters(paras_path)
        self.model_paras_is_loaded = True
        # except:
        #    logger.warning(&#34;Model parameters not successfully loaded.&#34;)
        logger.info(&#34;Model loaded.&#34;)

    def load_model_with_path(self, model_path, paras_path=None):
        &#34;&#34;&#34;Loads model with given path

        Note:
            Should load model parameters manually.
        &#34;&#34;&#34;
        self._model = keras.models.load_model(
            model_path, custom_objects={&#34;plain_acc&#34;: plain_acc},
        )  # it&#39;s important to specify
        if paras_path is not None:
            try:
                self.load_model_parameters(paras_path)
                self.model_paras_is_loaded = True
            except:
                logger.warning(&#34;Model parameters not successfully loaded.&#34;)
        logger.info(&#34;Model loaded.&#34;)

    def load_model_parameters(self, paras_path):
        &#34;&#34;&#34;Retrieves model parameters from yaml file.&#34;&#34;&#34;
        with open(paras_path, &#34;r&#34;) as paras_file:
            paras_dict = yaml.load(paras_file, Loader=yaml.UnsafeLoader)
        # sorted by alphabet
        self._job_config.update(paras_dict[&#34;job_config_dict&#34;])

        model_meta_save = paras_dict[&#34;model_meta&#34;]
        self._model_meta = model_meta_save
        self._model_name = model_meta_save[&#34;model_name&#34;]
        self._model_label = model_meta_save[&#34;model_label&#34;]
        self._model_note = model_meta_save[&#34;model_note&#34;]
        self._model_create_time = model_meta_save[&#34;model_create_time&#34;]
        self._model_is_compiled = model_meta_save[&#34;model_is_compiled&#34;]
        self._model_is_saved = model_meta_save[&#34;model_is_saved&#34;]
        self._model_is_trained = model_meta_save[&#34;model_is_trained&#34;]

        self._train_history = paras_dict[&#34;train_history&#34;]

    def save_model(self, save_dir=None, file_name=None):
        &#34;&#34;&#34;Saves trained model.

        Args:
            save_dir: str
            Path to save model.

        &#34;&#34;&#34;
        # Define save path
        if save_dir is None:
            save_dir = &#34;./models&#34;
        if file_name is None:
            datestr = datetime.date.today().strftime(&#34;%Y-%m-%d&#34;)
            file_name = self._model_name + &#34;_&#34; + self._model_label + &#34;_&#34; + datestr
        # Check path
        save_path = save_dir + &#34;/&#34; + file_name + &#34;.h5&#34;
        pathlib.Path(save_dir).mkdir(parents=True, exist_ok=True)
        # Save
        self._model.save(save_path)
        self._model_save_path = save_path
        logger.debug(f&#34;model: {self._model_name} has been saved to: {save_path}&#34;)
        # update path for yaml
        save_path = save_dir + &#34;/&#34; + file_name + &#34;_paras.yaml&#34;
        self.save_model_paras(save_path)
        logger.debug(f&#34;model parameters has been saved to: {save_path}&#34;)
        self._model_is_saved = True

    def save_model_paras(self, save_path):
        &#34;&#34;&#34;Save model parameters to yaml file.&#34;&#34;&#34;
        paras_dict = dict()
        paras_dict[&#34;job_config_dict&#34;] = self._job_config.get_config_dict()

        model_meta_save = copy.deepcopy(self._model_meta)
        model_meta_save[&#34;model_name&#34;] = self._model_name
        model_meta_save[&#34;model_label&#34;] = self._model_label
        model_meta_save[&#34;model_note&#34;] = self._model_note
        model_meta_save[&#34;model_create_time&#34;] = self._model_create_time
        model_meta_save[&#34;model_is_compiled&#34;] = self._model_is_compiled
        model_meta_save[&#34;model_is_saved&#34;] = self._model_is_saved
        model_meta_save[&#34;model_is_trained&#34;] = self._model_is_trained
        paras_dict[&#34;model_meta&#34;] = model_meta_save

        paras_dict[&#34;train_history&#34;] = self._train_history

        with open(save_path, &#34;w&#34;) as write_file:
            yaml.dump(paras_dict, write_file, indent=2)

    def set_inputs(self, job_config) -&gt; None:
        &#34;&#34;&#34;Prepares array for training.&#34;&#34;&#34;
        feedbox = feed_box.Feedbox(
            job_config,
            model_meta=self.get_model_meta(),
        )
        if job_config.job.job_type == &#34;apply&#34;:
            feedbox.load_sig_arrays()
            feedbox.load_bkg_arrays()
        self._model_meta[&#34;norm_dict&#34;] = copy.deepcopy(feedbox.get_norm_dict())
        self.feedbox = feedbox
        self._array_prepared = feedbox._array_prepared

    def train(
        self, job_config, model_save_dir=None, file_name=None,
    ):
        &#34;&#34;&#34;Performs training.&#34;&#34;&#34;

        # prepare config alias
        ic = self._job_config.input
        tc = self._job_config.train

        # Check
        if self._model_is_compiled == False:
            logging.critical(&#34;DNN model is not yet compiled&#34;)
            exit(1)
        if self._array_prepared == False:
            logging.critical(&#34;Training data is not ready.&#34;)
            exit(1)

        # Train
        logger.info(&#34;-&#34; * 40)
        logger.info(f&#34;Training start. Using model: {self._model_name}&#34;)
        logger.info(f&#34;Model info: {self._model_note}&#34;)
        self.class_weight = {1: tc.sig_class_weight, 0: tc.bkg_class_weight}
        ## setup callbacks
        train_callbacks = []
        if self._save_tb_logs:  # TODO: add back this function
            pass
            # if self._tb_logs_path is None:
            #    self._tb_logs_path = &#34;temp_logs/{}&#34;.format(self._model_label)
            #    logger.warning(
            #        &#34;TensorBoard logs path not specified, set path to: {}&#34;.format(
            #            self._tb_logs_path
            #        )
            #    )
            # tb_callback = TensorBoard(log_dir=self._tb_logs_path, histogram_freq=1)
            # train_callbacks.append(tb_callback)
        if tc.use_early_stop:
            early_stop_callback = callbacks.EarlyStopping(
                monitor=tc.early_stop_paras.monitor,
                min_delta=tc.early_stop_paras.min_delta,
                patience=tc.early_stop_paras.patience,
                mode=tc.early_stop_paras.mode,
                restore_best_weights=tc.early_stop_paras.restore_best_weights,
            )
            train_callbacks.append(early_stop_callback)
        ## set up check point to save model in each epoch
        if model_save_dir is None:
            model_save_dir = &#34;./models&#34;
        pathlib.Path(model_save_dir).mkdir(parents=True, exist_ok=True)
        if file_name is None:
            file_name = self._model_name
        path_pattern = model_save_dir + &#34;/&#34; + file_name + &#34;_epoch{epoch:03d}.h5&#34;
        checkpoint = ModelCheckpoint(path_pattern, monitor=&#34;val_loss&#34;)
        train_callbacks.append(checkpoint)
        ## check input
        train_test_dict = self.feedbox.get_train_test_arrays(
            sig_key=ic.sig_key,
            bkg_key=ic.bkg_key,
            multi_class_bkgs=tc.output_bkg_node_names,
            output_keys=train_utils.COMB_KEYS,
        )
        self.feedbox = None
        x_train = train_test_dict[&#34;x_train&#34;]
        x_test = train_test_dict[&#34;x_test&#34;]
        y_train = train_test_dict[&#34;y_train&#34;]
        y_test = train_test_dict[&#34;y_test&#34;]
        wt_train = train_test_dict[&#34;wt_train&#34;]
        wt_test = train_test_dict[&#34;wt_test&#34;]
        train_test_dict = None
        self.get_model().summary()
        ## train
        history_obj = self.get_model().fit(
            x_train,
            y_train,
            batch_size=tc.batch_size,
            epochs=tc.epochs,
            validation_split=tc.val_split,
            sample_weight=wt_train,
            callbacks=train_callbacks,
            verbose=tc.verbose,
        )
        self._train_history = history_obj.history
        logger.info(&#34;Training finished.&#34;)

        # Evaluation
        logger.info(&#34;Evaluate with test dataset:&#34;)
        score = self.get_model().evaluate(
            x_test, y_test, verbose=tc.verbose, sample_weight=wt_test,
        )

        if not isinstance(score, typing.Iterable):
            logger.info(f&#34;&gt; test loss: {score}&#34;)
        else:
            for i, metric in enumerate(self.get_model().metrics_names):
                logger.info(f&#34;&gt; test - {metric}: {score[i]}&#34;)

        # update status
        self._model_is_trained = True

    &#39;&#39;&#39;
    def tuning_train(
        self,
        sig_key=&#34;all&#34;,
        bkg_key=&#34;all&#34;,
        batch_size=128,
        epochs=20,
        val_split=0.25,
        sig_class_weight=1.0,
        bkg_class_weight=1.0,
        verbose=1,
    ):
        &#34;&#34;&#34;Performs quick training for hyperparameters tuning.&#34;&#34;&#34;
        # Check
        if self._model_is_compiled == False:
            raise ValueError(&#34;DNN model is not yet compiled&#34;)
        if self._array_prepared == False:
            raise ValueError(&#34;Training data is not ready.&#34;)
        # separate validation samples
        train_test_dict = self.feedbox.get_train_test_arrays(
            sig_key=sig_key,
            bkg_key=bkg_key,
            multi_class_bkgs=self.model_hypers[&#34;output_bkg_node_names&#34;],
            use_selected=False,
        )
        x_train = train_test_dict[&#34;x_train&#34;]
        y_train = train_test_dict[&#34;y_train&#34;]
        x_train_selected = train_test_dict[&#34;x_train_selected&#34;]
        num_val = math.ceil(len(y_train) * val_split)
        x_tr = x_train_selected[:-num_val, :]
        x_val = x_train_selected[-num_val:, :]
        y_tr = y_train[:-num_val]
        y_val = y_train[-num_val:]
        wt_tr = x_train[:-num_val, -1]
        wt_val = x_train[-num_val:, -1]
        val_tuple = (x_val, y_val, wt_val)
        self.x_tr = x_tr
        self.x_val = x_val
        self.y_tr = y_tr
        self.y_val = y_val
        self.wt_tr = wt_tr
        self.wt_val = wt_val
        # Train
        self.class_weight = {1: sig_class_weight, 0: bkg_class_weight}
        train_callbacks = []
        if self.model_hypers[&#34;use_early_stop&#34;]:
            early_stop_callback = callbacks.EarlyStopping(
                monitor=self.model_hypers[&#34;early_stop_paras&#34;][&#34;monitor&#34;],
                min_delta=self.model_hypers[&#34;early_stop_paras&#34;][&#34;min_delta&#34;],
                patience=self.model_hypers[&#34;early_stop_paras&#34;][&#34;patience&#34;],
                mode=self.model_hypers[&#34;early_stop_paras&#34;][&#34;mode&#34;],
                restore_best_weights=self.model_hypers[&#34;early_stop_paras&#34;][
                    &#34;restore_best_weights&#34;
                ],
            )
            train_callbacks.append(early_stop_callback)
        self._train_history = self.get_model().fit(
            x_tr,
            y_tr,
            batch_size=batch_size,
            epochs=epochs,
            validation_data=val_tuple,
            class_weight=self.class_weight,
            sample_weight=wt_tr,
            callbacks=train_callbacks,
            verbose=verbose,
        )
        # Final evaluation
        score = self.get_model().evaluate(
            x_val, y_val, verbose=verbose, sample_weight=wt_val
        )
        # update status
        self._model_is_trained = True
        return score[0]
    &#39;&#39;&#39;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="hepynet.train.model.Model_Base" href="#hepynet.train.model.Model_Base">Model_Base</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="hepynet.train.model.Model_Sequential_Flat" href="#hepynet.train.model.Model_Sequential_Flat">Model_Sequential_Flat</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="hepynet.train.model.Model_Sequential_Base.get_corrcoef"><code class="name flex">
<span>def <span class="ident">get_corrcoef</span></span>(<span>self) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_corrcoef(self) -&gt; dict:
    bkg_array, _ = self.feedbox.get_reweight(
        &#34;xb&#34;, array_key=&#34;all&#34;, reset_mass=False
    )
    d_bkg = pd.DataFrame(data=bkg_array, columns=list(self._selected_features),)
    bkg_matrix = d_bkg.corr()
    sig_array, _ = self.feedbox.get_reweight(
        &#34;xs&#34;, array_key=&#34;all&#34;, reset_mass=False
    )
    d_sig = pd.DataFrame(data=sig_array, columns=list(self._selected_features),)
    sig_matrix = d_sig.corr()
    corrcoef_matrix_dict = {}
    corrcoef_matrix_dict[&#34;bkg&#34;] = bkg_matrix
    corrcoef_matrix_dict[&#34;sig&#34;] = sig_matrix
    return corrcoef_matrix_dict</code></pre>
</details>
</dd>
<dt id="hepynet.train.model.Model_Sequential_Base.get_model"><code class="name flex">
<span>def <span class="ident">get_model</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_model(self):
    &#34;&#34;&#34;Returns model.&#34;&#34;&#34;
    if not self._model_is_compiled:
        logger.warning(&#34;Model is not compiled&#34;)
    return self._model</code></pre>
</details>
</dd>
<dt id="hepynet.train.model.Model_Sequential_Base.get_model_meta"><code class="name flex">
<span>def <span class="ident">get_model_meta</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_model_meta(self):
    return self._model_meta</code></pre>
</details>
</dd>
<dt id="hepynet.train.model.Model_Sequential_Base.get_train_history"><code class="name flex">
<span>def <span class="ident">get_train_history</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns train history.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_train_history(self):
    &#34;&#34;&#34;Returns train history.&#34;&#34;&#34;
    if not self._model_is_compiled:
        logger.warning(&#34;Model is not compiled&#34;)
    if self._train_history is None:
        logger.warning(&#34;Empty training history found&#34;)
    return self._train_history</code></pre>
</details>
</dd>
<dt id="hepynet.train.model.Model_Sequential_Base.get_train_performance_meta"><code class="name flex">
<span>def <span class="ident">get_train_performance_meta</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns meta data of training performance</p>
<h2 id="note">Note</h2>
<p>This function should be called after show_performance and
plot_significance_scan being called, otherwise "-" will be used as
content.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_train_performance_meta(self):
    &#34;&#34;&#34;Returns meta data of training performance

    Note:
        This function should be called after show_performance and
    plot_significance_scan being called, otherwise &#34;-&#34; will be used as
    content.

    &#34;&#34;&#34;
    performance_meta_dict = {}
    # try collect significance scan result
    try:
        performance_meta_dict[&#34;original_significance&#34;] = self.original_significance
        performance_meta_dict[&#34;max_significance&#34;] = self.max_significance
        performance_meta_dict[
            &#34;max_significance_threshold&#34;
        ] = self.max_significance_threshold
    except:
        performance_meta_dict[&#34;original_significance&#34;] = &#34;-&#34;
        performance_meta_dict[&#34;max_significance&#34;] = &#34;-&#34;
        performance_meta_dict[&#34;max_significance_threshold&#34;] = &#34;-&#34;
    # try collect auc value
    try:
        # performance_meta_dict[&#34;auc_train&#34;] = self.auc_train
        # performance_meta_dict[&#34;auc_test&#34;] = self.auc_test
        # performance_meta_dict[&#34;auc_train_original&#34;] = self.auc_train_original
        # performance_meta_dict[&#34;auc_test_original&#34;] = self.auc_test_original
        pass
    except:
        # performance_meta_dict[&#34;auc_train&#34;] = &#34;-&#34;
        # performance_meta_dict[&#34;auc_test&#34;] = &#34;-&#34;
        # performance_meta_dict[&#34;auc_train_original&#34;] = &#34;-&#34;
        # performance_meta_dict[&#34;auc_test_original&#34;] = &#34;-&#34;
        pass
    return performance_meta_dict</code></pre>
</details>
</dd>
<dt id="hepynet.train.model.Model_Sequential_Base.load_model"><code class="name flex">
<span>def <span class="ident">load_model</span></span>(<span>self, load_dir, _model_name, job_name='*', date='*', version='*')</span>
</code></dt>
<dd>
<div class="desc"><p>Loads saved model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_model(self, load_dir, _model_name, job_name=&#34;*&#34;, date=&#34;*&#34;, version=&#34;*&#34;):
    &#34;&#34;&#34;Loads saved model.&#34;&#34;&#34;
    # Search possible files
    search_pattern = (
        load_dir + &#34;/&#34; + date + &#34;_&#34; + job_name + &#34;_&#34; + version + &#34;/models&#34;
    )
    model_dir_list = glob.glob(search_pattern)
    if not model_dir_list:
        search_pattern = &#34;/work/&#34; + search_pattern
        logger.debug(f&#34;search pattern:{search_pattern}&#34;)
        model_dir_list = glob.glob(search_pattern)
    model_dir_list = sorted(model_dir_list)
    # Choose the newest one
    if len(model_dir_list) &lt; 1:
        raise FileNotFoundError(&#34;Model file that matched the pattern not found.&#34;)
    model_dir = model_dir_list[-1]
    if len(model_dir_list) &gt; 1:
        logger.info(
            &#34;More than one valid model file found, maybe you should try to specify more infomation.&#34;
        )
        logger.info(f&#34;Loading the last matched model path: {model_dir}&#34;)
    else:
        logger.info(f&#34;Loading model at: {model_dir}&#34;)
    self._model = keras.models.load_model(
        model_dir + &#34;/&#34; + _model_name + &#34;.h5&#34;,
        custom_objects={&#34;plain_acc&#34;: plain_acc},
    )  # it&#39;s important to specify
    # custom objects
    self._model_is_loaded = True
    # Load parameters
    # try:
    paras_path = model_dir + &#34;/&#34; + _model_name + &#34;_paras.yaml&#34;
    self.load_model_parameters(paras_path)
    self.model_paras_is_loaded = True
    # except:
    #    logger.warning(&#34;Model parameters not successfully loaded.&#34;)
    logger.info(&#34;Model loaded.&#34;)</code></pre>
</details>
</dd>
<dt id="hepynet.train.model.Model_Sequential_Base.load_model_parameters"><code class="name flex">
<span>def <span class="ident">load_model_parameters</span></span>(<span>self, paras_path)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves model parameters from yaml file.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_model_parameters(self, paras_path):
    &#34;&#34;&#34;Retrieves model parameters from yaml file.&#34;&#34;&#34;
    with open(paras_path, &#34;r&#34;) as paras_file:
        paras_dict = yaml.load(paras_file, Loader=yaml.UnsafeLoader)
    # sorted by alphabet
    self._job_config.update(paras_dict[&#34;job_config_dict&#34;])

    model_meta_save = paras_dict[&#34;model_meta&#34;]
    self._model_meta = model_meta_save
    self._model_name = model_meta_save[&#34;model_name&#34;]
    self._model_label = model_meta_save[&#34;model_label&#34;]
    self._model_note = model_meta_save[&#34;model_note&#34;]
    self._model_create_time = model_meta_save[&#34;model_create_time&#34;]
    self._model_is_compiled = model_meta_save[&#34;model_is_compiled&#34;]
    self._model_is_saved = model_meta_save[&#34;model_is_saved&#34;]
    self._model_is_trained = model_meta_save[&#34;model_is_trained&#34;]

    self._train_history = paras_dict[&#34;train_history&#34;]</code></pre>
</details>
</dd>
<dt id="hepynet.train.model.Model_Sequential_Base.load_model_with_path"><code class="name flex">
<span>def <span class="ident">load_model_with_path</span></span>(<span>self, model_path, paras_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads model with given path</p>
<h2 id="note">Note</h2>
<p>Should load model parameters manually.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_model_with_path(self, model_path, paras_path=None):
    &#34;&#34;&#34;Loads model with given path

    Note:
        Should load model parameters manually.
    &#34;&#34;&#34;
    self._model = keras.models.load_model(
        model_path, custom_objects={&#34;plain_acc&#34;: plain_acc},
    )  # it&#39;s important to specify
    if paras_path is not None:
        try:
            self.load_model_parameters(paras_path)
            self.model_paras_is_loaded = True
        except:
            logger.warning(&#34;Model parameters not successfully loaded.&#34;)
    logger.info(&#34;Model loaded.&#34;)</code></pre>
</details>
</dd>
<dt id="hepynet.train.model.Model_Sequential_Base.save_model"><code class="name flex">
<span>def <span class="ident">save_model</span></span>(<span>self, save_dir=None, file_name=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves trained model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>save_dir</code></strong></dt>
<dd>str</dd>
</dl>
<p>Path to save model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_model(self, save_dir=None, file_name=None):
    &#34;&#34;&#34;Saves trained model.

    Args:
        save_dir: str
        Path to save model.

    &#34;&#34;&#34;
    # Define save path
    if save_dir is None:
        save_dir = &#34;./models&#34;
    if file_name is None:
        datestr = datetime.date.today().strftime(&#34;%Y-%m-%d&#34;)
        file_name = self._model_name + &#34;_&#34; + self._model_label + &#34;_&#34; + datestr
    # Check path
    save_path = save_dir + &#34;/&#34; + file_name + &#34;.h5&#34;
    pathlib.Path(save_dir).mkdir(parents=True, exist_ok=True)
    # Save
    self._model.save(save_path)
    self._model_save_path = save_path
    logger.debug(f&#34;model: {self._model_name} has been saved to: {save_path}&#34;)
    # update path for yaml
    save_path = save_dir + &#34;/&#34; + file_name + &#34;_paras.yaml&#34;
    self.save_model_paras(save_path)
    logger.debug(f&#34;model parameters has been saved to: {save_path}&#34;)
    self._model_is_saved = True</code></pre>
</details>
</dd>
<dt id="hepynet.train.model.Model_Sequential_Base.save_model_paras"><code class="name flex">
<span>def <span class="ident">save_model_paras</span></span>(<span>self, save_path)</span>
</code></dt>
<dd>
<div class="desc"><p>Save model parameters to yaml file.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_model_paras(self, save_path):
    &#34;&#34;&#34;Save model parameters to yaml file.&#34;&#34;&#34;
    paras_dict = dict()
    paras_dict[&#34;job_config_dict&#34;] = self._job_config.get_config_dict()

    model_meta_save = copy.deepcopy(self._model_meta)
    model_meta_save[&#34;model_name&#34;] = self._model_name
    model_meta_save[&#34;model_label&#34;] = self._model_label
    model_meta_save[&#34;model_note&#34;] = self._model_note
    model_meta_save[&#34;model_create_time&#34;] = self._model_create_time
    model_meta_save[&#34;model_is_compiled&#34;] = self._model_is_compiled
    model_meta_save[&#34;model_is_saved&#34;] = self._model_is_saved
    model_meta_save[&#34;model_is_trained&#34;] = self._model_is_trained
    paras_dict[&#34;model_meta&#34;] = model_meta_save

    paras_dict[&#34;train_history&#34;] = self._train_history

    with open(save_path, &#34;w&#34;) as write_file:
        yaml.dump(paras_dict, write_file, indent=2)</code></pre>
</details>
</dd>
<dt id="hepynet.train.model.Model_Sequential_Base.set_inputs"><code class="name flex">
<span>def <span class="ident">set_inputs</span></span>(<span>self, job_config) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Prepares array for training.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_inputs(self, job_config) -&gt; None:
    &#34;&#34;&#34;Prepares array for training.&#34;&#34;&#34;
    feedbox = feed_box.Feedbox(
        job_config,
        model_meta=self.get_model_meta(),
    )
    if job_config.job.job_type == &#34;apply&#34;:
        feedbox.load_sig_arrays()
        feedbox.load_bkg_arrays()
    self._model_meta[&#34;norm_dict&#34;] = copy.deepcopy(feedbox.get_norm_dict())
    self.feedbox = feedbox
    self._array_prepared = feedbox._array_prepared</code></pre>
</details>
</dd>
<dt id="hepynet.train.model.Model_Sequential_Base.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, job_config, model_save_dir=None, file_name=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs training.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(
    self, job_config, model_save_dir=None, file_name=None,
):
    &#34;&#34;&#34;Performs training.&#34;&#34;&#34;

    # prepare config alias
    ic = self._job_config.input
    tc = self._job_config.train

    # Check
    if self._model_is_compiled == False:
        logging.critical(&#34;DNN model is not yet compiled&#34;)
        exit(1)
    if self._array_prepared == False:
        logging.critical(&#34;Training data is not ready.&#34;)
        exit(1)

    # Train
    logger.info(&#34;-&#34; * 40)
    logger.info(f&#34;Training start. Using model: {self._model_name}&#34;)
    logger.info(f&#34;Model info: {self._model_note}&#34;)
    self.class_weight = {1: tc.sig_class_weight, 0: tc.bkg_class_weight}
    ## setup callbacks
    train_callbacks = []
    if self._save_tb_logs:  # TODO: add back this function
        pass
        # if self._tb_logs_path is None:
        #    self._tb_logs_path = &#34;temp_logs/{}&#34;.format(self._model_label)
        #    logger.warning(
        #        &#34;TensorBoard logs path not specified, set path to: {}&#34;.format(
        #            self._tb_logs_path
        #        )
        #    )
        # tb_callback = TensorBoard(log_dir=self._tb_logs_path, histogram_freq=1)
        # train_callbacks.append(tb_callback)
    if tc.use_early_stop:
        early_stop_callback = callbacks.EarlyStopping(
            monitor=tc.early_stop_paras.monitor,
            min_delta=tc.early_stop_paras.min_delta,
            patience=tc.early_stop_paras.patience,
            mode=tc.early_stop_paras.mode,
            restore_best_weights=tc.early_stop_paras.restore_best_weights,
        )
        train_callbacks.append(early_stop_callback)
    ## set up check point to save model in each epoch
    if model_save_dir is None:
        model_save_dir = &#34;./models&#34;
    pathlib.Path(model_save_dir).mkdir(parents=True, exist_ok=True)
    if file_name is None:
        file_name = self._model_name
    path_pattern = model_save_dir + &#34;/&#34; + file_name + &#34;_epoch{epoch:03d}.h5&#34;
    checkpoint = ModelCheckpoint(path_pattern, monitor=&#34;val_loss&#34;)
    train_callbacks.append(checkpoint)
    ## check input
    train_test_dict = self.feedbox.get_train_test_arrays(
        sig_key=ic.sig_key,
        bkg_key=ic.bkg_key,
        multi_class_bkgs=tc.output_bkg_node_names,
        output_keys=train_utils.COMB_KEYS,
    )
    self.feedbox = None
    x_train = train_test_dict[&#34;x_train&#34;]
    x_test = train_test_dict[&#34;x_test&#34;]
    y_train = train_test_dict[&#34;y_train&#34;]
    y_test = train_test_dict[&#34;y_test&#34;]
    wt_train = train_test_dict[&#34;wt_train&#34;]
    wt_test = train_test_dict[&#34;wt_test&#34;]
    train_test_dict = None
    self.get_model().summary()
    ## train
    history_obj = self.get_model().fit(
        x_train,
        y_train,
        batch_size=tc.batch_size,
        epochs=tc.epochs,
        validation_split=tc.val_split,
        sample_weight=wt_train,
        callbacks=train_callbacks,
        verbose=tc.verbose,
    )
    self._train_history = history_obj.history
    logger.info(&#34;Training finished.&#34;)

    # Evaluation
    logger.info(&#34;Evaluate with test dataset:&#34;)
    score = self.get_model().evaluate(
        x_test, y_test, verbose=tc.verbose, sample_weight=wt_test,
    )

    if not isinstance(score, typing.Iterable):
        logger.info(f&#34;&gt; test loss: {score}&#34;)
    else:
        for i, metric in enumerate(self.get_model().metrics_names):
            logger.info(f&#34;&gt; test - {metric}: {score[i]}&#34;)

    # update status
    self._model_is_trained = True</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="hepynet.train.model.Model_Sequential_Flat"><code class="flex name class">
<span>class <span class="ident">Model_Sequential_Flat</span></span>
<span>(</span><span>job_config)</span>
</code></dt>
<dd>
<div class="desc"><p>Sequential model optimized with old ntuple at Sep. 9th 2019.</p>
<p>Major modification based on 1002 model:
1. Change structure to make quantity of nodes decrease with layer num.</p>
<p>Initialize model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Model_Sequential_Flat(Model_Sequential_Base):
    &#34;&#34;&#34;Sequential model optimized with old ntuple at Sep. 9th 2019.

    Major modification based on 1002 model:
        1. Change structure to make quantity of nodes decrease with layer num.

    &#34;&#34;&#34;

    def __init__(
        self, job_config,
    ):
        super().__init__(job_config)

        self._model_label = &#34;mod_seq&#34;
        self._model_note = &#34;Sequential model with flexible layers and nodes.&#34;

    def compile(self):
        &#34;&#34;&#34; Compile model, function to be changed in the future.&#34;&#34;&#34;
        tc = self._job_config.train
        # Add layers
        for layer in range(tc.layers):
            # input layer
            if layer == 0:
                self._model.add(
                    Dense(
                        tc.nodes,
                        kernel_initializer=&#34;glorot_uniform&#34;,
                        activation=&#34;relu&#34;,
                        input_dim=self._model_input_dim,
                    )
                )
            # hidden layers
            else:
                self._model.add(
                    Dense(
                        tc.nodes,
                        kernel_initializer=&#34;glorot_uniform&#34;,
                        activation=&#34;relu&#34;,
                    )
                )
            if tc.dropout_rate != 0:
                self._model.add(Dropout(tc.dropout_rate))
        # output layer
        if tc.output_bkg_node_names:
            num_nodes_out = len(tc.output_bkg_node_names) + 1
        else:
            num_nodes_out = 1
        self._model.add(
            Dense(
                num_nodes_out,
                kernel_initializer=&#34;glorot_uniform&#34;,
                activation=&#34;sigmoid&#34;,
            )
        )
        # Compile
        # transfer self-defined metrics into real function
        metrics = copy.deepcopy(tc.train_metrics)
        weighted_metrics = copy.deepcopy(tc.train_metrics_weighted)
        if metrics is not None and &#34;plain_acc&#34; in metrics:
            index = metrics.index(&#34;plain_acc&#34;)
            metrics[index] = plain_acc
        if weighted_metrics is not None and &#34;plain_acc&#34; in weighted_metrics:
            index = weighted_metrics.index(&#34;plain_acc&#34;)
            weighted_metrics[index] = plain_acc
        # compile model
        self._model.compile(
            loss=&#34;binary_crossentropy&#34;,
            optimizer=SGD(
                lr=tc.learn_rate,
                decay=tc.learn_rate_decay,
                momentum=tc.momentum,
                nesterov=tc.nesterov,
            ),
            metrics=metrics,
            weighted_metrics=weighted_metrics,
        )
        self._model_is_compiled = True</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="hepynet.train.model.Model_Sequential_Base" href="#hepynet.train.model.Model_Sequential_Base">Model_Sequential_Base</a></li>
<li><a title="hepynet.train.model.Model_Base" href="#hepynet.train.model.Model_Base">Model_Base</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="hepynet.train.model.Model_Sequential_Flat.compile"><code class="name flex">
<span>def <span class="ident">compile</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Compile model, function to be changed in the future.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile(self):
    &#34;&#34;&#34; Compile model, function to be changed in the future.&#34;&#34;&#34;
    tc = self._job_config.train
    # Add layers
    for layer in range(tc.layers):
        # input layer
        if layer == 0:
            self._model.add(
                Dense(
                    tc.nodes,
                    kernel_initializer=&#34;glorot_uniform&#34;,
                    activation=&#34;relu&#34;,
                    input_dim=self._model_input_dim,
                )
            )
        # hidden layers
        else:
            self._model.add(
                Dense(
                    tc.nodes,
                    kernel_initializer=&#34;glorot_uniform&#34;,
                    activation=&#34;relu&#34;,
                )
            )
        if tc.dropout_rate != 0:
            self._model.add(Dropout(tc.dropout_rate))
    # output layer
    if tc.output_bkg_node_names:
        num_nodes_out = len(tc.output_bkg_node_names) + 1
    else:
        num_nodes_out = 1
    self._model.add(
        Dense(
            num_nodes_out,
            kernel_initializer=&#34;glorot_uniform&#34;,
            activation=&#34;sigmoid&#34;,
        )
    )
    # Compile
    # transfer self-defined metrics into real function
    metrics = copy.deepcopy(tc.train_metrics)
    weighted_metrics = copy.deepcopy(tc.train_metrics_weighted)
    if metrics is not None and &#34;plain_acc&#34; in metrics:
        index = metrics.index(&#34;plain_acc&#34;)
        metrics[index] = plain_acc
    if weighted_metrics is not None and &#34;plain_acc&#34; in weighted_metrics:
        index = weighted_metrics.index(&#34;plain_acc&#34;)
        weighted_metrics[index] = plain_acc
    # compile model
    self._model.compile(
        loss=&#34;binary_crossentropy&#34;,
        optimizer=SGD(
            lr=tc.learn_rate,
            decay=tc.learn_rate_decay,
            momentum=tc.momentum,
            nesterov=tc.nesterov,
        ),
        metrics=metrics,
        weighted_metrics=weighted_metrics,
    )
    self._model_is_compiled = True</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="hepynet.train.model.Model_Sequential_Base" href="#hepynet.train.model.Model_Sequential_Base">Model_Sequential_Base</a></b></code>:
<ul class="hlist">
<li><code><a title="hepynet.train.model.Model_Sequential_Base.get_model" href="#hepynet.train.model.Model_Sequential_Base.get_model">get_model</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.get_train_history" href="#hepynet.train.model.Model_Sequential_Base.get_train_history">get_train_history</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.get_train_performance_meta" href="#hepynet.train.model.Model_Sequential_Base.get_train_performance_meta">get_train_performance_meta</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.load_model" href="#hepynet.train.model.Model_Sequential_Base.load_model">load_model</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.load_model_parameters" href="#hepynet.train.model.Model_Sequential_Base.load_model_parameters">load_model_parameters</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.load_model_with_path" href="#hepynet.train.model.Model_Sequential_Base.load_model_with_path">load_model_with_path</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.save_model" href="#hepynet.train.model.Model_Sequential_Base.save_model">save_model</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.save_model_paras" href="#hepynet.train.model.Model_Sequential_Base.save_model_paras">save_model_paras</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.set_inputs" href="#hepynet.train.model.Model_Sequential_Base.set_inputs">set_inputs</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.train" href="#hepynet.train.model.Model_Sequential_Base.train">train</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="hepynet.train" href="index.html">hepynet.train</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="hepynet.train.model.get_model_class" href="#hepynet.train.model.get_model_class">get_model_class</a></code></li>
<li><code><a title="hepynet.train.model.plain_acc" href="#hepynet.train.model.plain_acc">plain_acc</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="hepynet.train.model.Model_Base" href="#hepynet.train.model.Model_Base">Model_Base</a></code></h4>
</li>
<li>
<h4><code><a title="hepynet.train.model.Model_Sequential_Base" href="#hepynet.train.model.Model_Sequential_Base">Model_Sequential_Base</a></code></h4>
<ul class="">
<li><code><a title="hepynet.train.model.Model_Sequential_Base.get_corrcoef" href="#hepynet.train.model.Model_Sequential_Base.get_corrcoef">get_corrcoef</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.get_model" href="#hepynet.train.model.Model_Sequential_Base.get_model">get_model</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.get_model_meta" href="#hepynet.train.model.Model_Sequential_Base.get_model_meta">get_model_meta</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.get_train_history" href="#hepynet.train.model.Model_Sequential_Base.get_train_history">get_train_history</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.get_train_performance_meta" href="#hepynet.train.model.Model_Sequential_Base.get_train_performance_meta">get_train_performance_meta</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.load_model" href="#hepynet.train.model.Model_Sequential_Base.load_model">load_model</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.load_model_parameters" href="#hepynet.train.model.Model_Sequential_Base.load_model_parameters">load_model_parameters</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.load_model_with_path" href="#hepynet.train.model.Model_Sequential_Base.load_model_with_path">load_model_with_path</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.save_model" href="#hepynet.train.model.Model_Sequential_Base.save_model">save_model</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.save_model_paras" href="#hepynet.train.model.Model_Sequential_Base.save_model_paras">save_model_paras</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.set_inputs" href="#hepynet.train.model.Model_Sequential_Base.set_inputs">set_inputs</a></code></li>
<li><code><a title="hepynet.train.model.Model_Sequential_Base.train" href="#hepynet.train.model.Model_Sequential_Base.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="hepynet.train.model.Model_Sequential_Flat" href="#hepynet.train.model.Model_Sequential_Flat">Model_Sequential_Flat</a></code></h4>
<ul class="">
<li><code><a title="hepynet.train.model.Model_Sequential_Flat.compile" href="#hepynet.train.model.Model_Sequential_Flat.compile">compile</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>