<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>hepynet.train.train_utils API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>hepynet.train.train_utils</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
import logging
import math
import os
import pathlib
from typing import List, Optional

import numpy as np
import ray
import yaml
from ray import tune
from ray.tune import schedulers, stopper
from sklearn.model_selection import StratifiedKFold

import hepynet.common.hepy_type as ht
from hepynet.train import hep_model

logger = logging.getLogger(&#34;hepynet&#34;)


def get_model_class(model_class: str):
    if model_class == &#34;Model_Sequential_Flat&#34;:
        return hep_model.Model_Sequential_Flat
    else:
        logger.critical(f&#34;Unknown model class: {model_class}&#34;)
        exit(1)


def get_mean_var(
    array: np.ndarray,
    axis: Optional[int] = None,
    weights: Optional[np.ndarray] = None,
):
    &#34;&#34;&#34;Calculate average and variance of an array.&#34;&#34;&#34;
    average = np.average(array, axis=axis, weights=weights)
    variance = np.average((array - average) ** 2, axis=axis, weights=weights)
    if 0 in variance:
        logger.warn(&#34;Encountered 0 variance, adding shift value 0.000001&#34;)
    return average, variance + 0.000001


def get_single_hyper(hyper_config: ht.sub_config):
    &#34;&#34;&#34;Determines one hyperparameter
    
    Note:
        If the hyper-parameter is a tuning dimension (with spacer specified), 
        spacer from ray.tune will be used to set up the dimension.
        If the hyper-parameter is a normal value, it will be returned directly.

    &#34;&#34;&#34;
    if hasattr(hyper_config, &#34;spacer&#34;):
        paras = hyper_config.paras.get_config_dict()
        return getattr(tune, hyper_config.spacer)(**paras)
    else:
        return hyper_config


def get_train_val_indices(
    x: np.ndarray,
    y: np.ndarray,
    wt: np.ndarray,
    val_split: float,
    k_folds: Optional[int] = None,
):
    &#34;&#34;&#34;Gets indices to separates train datasets to train/validation&#34;&#34;&#34;
    train_indices_list = list()
    validation_indices_list = list()
    if isinstance(k_folds, int) and k_folds &gt;= 2:
        # skf = KFold(n_splits=k_folds, shuffle=True)
        skf = StratifiedKFold(n_splits=k_folds, shuffle=True)
        for train_index, val_index in skf.split(x, y):
            np.random.shuffle(train_index)
            np.random.shuffle(val_index)
            train_indices_list.append(train_index)
            validation_indices_list.append(val_index)
    else:
        if isinstance(k_folds, int) and k_folds &lt;= 2:
            logger.error(
                f&#34;Invalid train.k_folds value {k_folds} detected, will not use k-fold validation&#34;
            )
        array_len = len(wt)
        val_index = np.random.choice(
            range(array_len), int(array_len * 1.0 * val_split), replace=False
        )
        train_index = np.setdiff1d(np.array(range(array_len)), val_index)
        np.random.shuffle(train_index)
        np.random.shuffle(val_index)
        train_indices_list.append(train_index)
        validation_indices_list.append(val_index)
    return train_indices_list, validation_indices_list


def merge_unequal_length_arrays(array_list: List[np.ndarray]):
    &#34;&#34;&#34;Merges arrays with unequal length to average/min/max

    Note:
        mainly used to deal with k-fold results with early-stopping (which
        results in unequal length of results as different folds may stop at
        different epochs) enabled

    &#34;&#34;&#34;
    folds_lengths = list()
    for single_array in array_list:
        folds_lengths.append(len(single_array))
    max_len = max(folds_lengths)

    mean = list()
    low = list()
    high = list()
    for i in range(max_len):
        sum_value = 0
        min_value = math.inf
        max_value = -math.inf
        num_values = 0
        for fold_num, single_array in enumerate(array_list):
            if i &lt; folds_lengths[fold_num]:
                ele = single_array[i]
                sum_value += ele
                num_values += 1
                if ele &lt; min_value:
                    min_value = ele
                if ele &gt; max_value:
                    max_value = ele
        mean_value = sum_value / num_values
        mean.append(mean_value)
        low.append(min_value)
        high.append(max_value)
    return mean, low, high


def ray_tune(model_wrapper, job_config: ht.config, resume: bool = False):
    &#34;&#34;&#34;Performs automatic hyper-parameters tuning with Ray&#34;&#34;&#34;
    # initialize
    tuner = job_config.tune.clone().tuner
    log_dir = pathlib.Path(job_config.run.save_sub_dir) / &#34;tmp_log&#34;
    log_dir.mkdir(parents=True, exist_ok=True)
    if os.name == &#34;posix&#34;:
        logger.info(f&#34;Ignoring tune.tmp.tmp_dir setting on Unix OS&#34;)
        ray.init(**(tuner.init.get_config_dict()))
    else:
        ray.init(
            _temp_dir=str(job_config.tune.tmp_dir),
            **(tuner.init.get_config_dict()),
        )
    # set up scheduler
    sched_class = getattr(schedulers, tuner.scheduler_class)
    logger.info(f&#34;Setting up scheduler: {tuner.scheduler_class}&#34;)
    sched_config = tuner.scheduler.get_config_dict()
    sched = sched_class(**sched_config)
    # set up algorithm
    algo_class = tuner.algo_class
    logger.info(f&#34;Setting up search algorithm: {tuner.algo_class}&#34;)
    algo_config = tuner.algo.get_config_dict()
    algo = None
    if algo_class is None:
        algo = None
    elif algo_class == &#34;AxSearch&#34;:
        from ray.tune.suggest.ax import AxSearch

        algo = AxSearch(**algo_config)
    elif algo_class == &#34;HyperOptSearch&#34;:
        from ray.tune.suggest.hyperopt import HyperOptSearch

        algo = HyperOptSearch(**algo_config)
    elif algo_class == &#34;HEBOSearch&#34;:
        from ray.tune.suggest.hebo import HEBOSearch

        algo = HEBOSearch(**algo_config)
    else:
        logger.error(f&#34;Unsupported search algorithm: {algo_class}&#34;)
        logger.info(f&#34;Using default value None for search algorithm&#34;)
    # set stopper
    if tuner.stopper_class is None:
        stop = None
    else:
        stop = getattr(stopper, tuner.stopper_class)(**tuner.stopper)
    # run
    run_config = (
        tuner.run.get_config_dict()
    )  # important: convert Hepy_Config class to dict
    tune_func = getattr(hep_model, model_wrapper._tune_fun_name)
    analysis = tune.run(
        tune_func,
        name=&#34;ray_tunes&#34;,
        stop=stop,
        search_alg=algo,
        scheduler=sched,
        config=model_wrapper.get_hypers_tune(),
        local_dir=job_config.run.save_sub_dir,
        resume=resume,
        **run_config,
    )
    logger.info(&#34;Best hyperparameters found were:&#34;)
    print(yaml.dump(analysis.best_config))

    return analysis</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="hepynet.train.train_utils.get_mean_var"><code class="name flex">
<span>def <span class="ident">get_mean_var</span></span>(<span>array: numpy.ndarray, axis: Union[int, NoneType] = None, weights: Union[numpy.ndarray, NoneType] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate average and variance of an array.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_mean_var(
    array: np.ndarray,
    axis: Optional[int] = None,
    weights: Optional[np.ndarray] = None,
):
    &#34;&#34;&#34;Calculate average and variance of an array.&#34;&#34;&#34;
    average = np.average(array, axis=axis, weights=weights)
    variance = np.average((array - average) ** 2, axis=axis, weights=weights)
    if 0 in variance:
        logger.warn(&#34;Encountered 0 variance, adding shift value 0.000001&#34;)
    return average, variance + 0.000001</code></pre>
</details>
</dd>
<dt id="hepynet.train.train_utils.get_model_class"><code class="name flex">
<span>def <span class="ident">get_model_class</span></span>(<span>model_class: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_model_class(model_class: str):
    if model_class == &#34;Model_Sequential_Flat&#34;:
        return hep_model.Model_Sequential_Flat
    else:
        logger.critical(f&#34;Unknown model class: {model_class}&#34;)
        exit(1)</code></pre>
</details>
</dd>
<dt id="hepynet.train.train_utils.get_single_hyper"><code class="name flex">
<span>def <span class="ident">get_single_hyper</span></span>(<span>hyper_config: <a title="hepynet.common.config_utils.Hepy_Config_Section" href="../common/config_utils.html#hepynet.common.config_utils.Hepy_Config_Section">Hepy_Config_Section</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Determines one hyperparameter</p>
<h2 id="note">Note</h2>
<p>If the hyper-parameter is a tuning dimension (with spacer specified),
spacer from ray.tune will be used to set up the dimension.
If the hyper-parameter is a normal value, it will be returned directly.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_single_hyper(hyper_config: ht.sub_config):
    &#34;&#34;&#34;Determines one hyperparameter
    
    Note:
        If the hyper-parameter is a tuning dimension (with spacer specified), 
        spacer from ray.tune will be used to set up the dimension.
        If the hyper-parameter is a normal value, it will be returned directly.

    &#34;&#34;&#34;
    if hasattr(hyper_config, &#34;spacer&#34;):
        paras = hyper_config.paras.get_config_dict()
        return getattr(tune, hyper_config.spacer)(**paras)
    else:
        return hyper_config</code></pre>
</details>
</dd>
<dt id="hepynet.train.train_utils.get_train_val_indices"><code class="name flex">
<span>def <span class="ident">get_train_val_indices</span></span>(<span>x: numpy.ndarray, y: numpy.ndarray, wt: numpy.ndarray, val_split: float, k_folds: Union[int, NoneType] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets indices to separates train datasets to train/validation</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_train_val_indices(
    x: np.ndarray,
    y: np.ndarray,
    wt: np.ndarray,
    val_split: float,
    k_folds: Optional[int] = None,
):
    &#34;&#34;&#34;Gets indices to separates train datasets to train/validation&#34;&#34;&#34;
    train_indices_list = list()
    validation_indices_list = list()
    if isinstance(k_folds, int) and k_folds &gt;= 2:
        # skf = KFold(n_splits=k_folds, shuffle=True)
        skf = StratifiedKFold(n_splits=k_folds, shuffle=True)
        for train_index, val_index in skf.split(x, y):
            np.random.shuffle(train_index)
            np.random.shuffle(val_index)
            train_indices_list.append(train_index)
            validation_indices_list.append(val_index)
    else:
        if isinstance(k_folds, int) and k_folds &lt;= 2:
            logger.error(
                f&#34;Invalid train.k_folds value {k_folds} detected, will not use k-fold validation&#34;
            )
        array_len = len(wt)
        val_index = np.random.choice(
            range(array_len), int(array_len * 1.0 * val_split), replace=False
        )
        train_index = np.setdiff1d(np.array(range(array_len)), val_index)
        np.random.shuffle(train_index)
        np.random.shuffle(val_index)
        train_indices_list.append(train_index)
        validation_indices_list.append(val_index)
    return train_indices_list, validation_indices_list</code></pre>
</details>
</dd>
<dt id="hepynet.train.train_utils.merge_unequal_length_arrays"><code class="name flex">
<span>def <span class="ident">merge_unequal_length_arrays</span></span>(<span>array_list: List[numpy.ndarray])</span>
</code></dt>
<dd>
<div class="desc"><p>Merges arrays with unequal length to average/min/max</p>
<h2 id="note">Note</h2>
<p>mainly used to deal with k-fold results with early-stopping (which
results in unequal length of results as different folds may stop at
different epochs) enabled</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_unequal_length_arrays(array_list: List[np.ndarray]):
    &#34;&#34;&#34;Merges arrays with unequal length to average/min/max

    Note:
        mainly used to deal with k-fold results with early-stopping (which
        results in unequal length of results as different folds may stop at
        different epochs) enabled

    &#34;&#34;&#34;
    folds_lengths = list()
    for single_array in array_list:
        folds_lengths.append(len(single_array))
    max_len = max(folds_lengths)

    mean = list()
    low = list()
    high = list()
    for i in range(max_len):
        sum_value = 0
        min_value = math.inf
        max_value = -math.inf
        num_values = 0
        for fold_num, single_array in enumerate(array_list):
            if i &lt; folds_lengths[fold_num]:
                ele = single_array[i]
                sum_value += ele
                num_values += 1
                if ele &lt; min_value:
                    min_value = ele
                if ele &gt; max_value:
                    max_value = ele
        mean_value = sum_value / num_values
        mean.append(mean_value)
        low.append(min_value)
        high.append(max_value)
    return mean, low, high</code></pre>
</details>
</dd>
<dt id="hepynet.train.train_utils.ray_tune"><code class="name flex">
<span>def <span class="ident">ray_tune</span></span>(<span>model_wrapper, job_config: <a title="hepynet.common.config_utils.Hepy_Config" href="../common/config_utils.html#hepynet.common.config_utils.Hepy_Config">Hepy_Config</a>, resume: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs automatic hyper-parameters tuning with Ray</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ray_tune(model_wrapper, job_config: ht.config, resume: bool = False):
    &#34;&#34;&#34;Performs automatic hyper-parameters tuning with Ray&#34;&#34;&#34;
    # initialize
    tuner = job_config.tune.clone().tuner
    log_dir = pathlib.Path(job_config.run.save_sub_dir) / &#34;tmp_log&#34;
    log_dir.mkdir(parents=True, exist_ok=True)
    if os.name == &#34;posix&#34;:
        logger.info(f&#34;Ignoring tune.tmp.tmp_dir setting on Unix OS&#34;)
        ray.init(**(tuner.init.get_config_dict()))
    else:
        ray.init(
            _temp_dir=str(job_config.tune.tmp_dir),
            **(tuner.init.get_config_dict()),
        )
    # set up scheduler
    sched_class = getattr(schedulers, tuner.scheduler_class)
    logger.info(f&#34;Setting up scheduler: {tuner.scheduler_class}&#34;)
    sched_config = tuner.scheduler.get_config_dict()
    sched = sched_class(**sched_config)
    # set up algorithm
    algo_class = tuner.algo_class
    logger.info(f&#34;Setting up search algorithm: {tuner.algo_class}&#34;)
    algo_config = tuner.algo.get_config_dict()
    algo = None
    if algo_class is None:
        algo = None
    elif algo_class == &#34;AxSearch&#34;:
        from ray.tune.suggest.ax import AxSearch

        algo = AxSearch(**algo_config)
    elif algo_class == &#34;HyperOptSearch&#34;:
        from ray.tune.suggest.hyperopt import HyperOptSearch

        algo = HyperOptSearch(**algo_config)
    elif algo_class == &#34;HEBOSearch&#34;:
        from ray.tune.suggest.hebo import HEBOSearch

        algo = HEBOSearch(**algo_config)
    else:
        logger.error(f&#34;Unsupported search algorithm: {algo_class}&#34;)
        logger.info(f&#34;Using default value None for search algorithm&#34;)
    # set stopper
    if tuner.stopper_class is None:
        stop = None
    else:
        stop = getattr(stopper, tuner.stopper_class)(**tuner.stopper)
    # run
    run_config = (
        tuner.run.get_config_dict()
    )  # important: convert Hepy_Config class to dict
    tune_func = getattr(hep_model, model_wrapper._tune_fun_name)
    analysis = tune.run(
        tune_func,
        name=&#34;ray_tunes&#34;,
        stop=stop,
        search_alg=algo,
        scheduler=sched,
        config=model_wrapper.get_hypers_tune(),
        local_dir=job_config.run.save_sub_dir,
        resume=resume,
        **run_config,
    )
    logger.info(&#34;Best hyperparameters found were:&#34;)
    print(yaml.dump(analysis.best_config))

    return analysis</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="hepynet.train" href="index.html">hepynet.train</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="hepynet.train.train_utils.get_mean_var" href="#hepynet.train.train_utils.get_mean_var">get_mean_var</a></code></li>
<li><code><a title="hepynet.train.train_utils.get_model_class" href="#hepynet.train.train_utils.get_model_class">get_model_class</a></code></li>
<li><code><a title="hepynet.train.train_utils.get_single_hyper" href="#hepynet.train.train_utils.get_single_hyper">get_single_hyper</a></code></li>
<li><code><a title="hepynet.train.train_utils.get_train_val_indices" href="#hepynet.train.train_utils.get_train_val_indices">get_train_val_indices</a></code></li>
<li><code><a title="hepynet.train.train_utils.merge_unequal_length_arrays" href="#hepynet.train.train_utils.merge_unequal_length_arrays">merge_unequal_length_arrays</a></code></li>
<li><code><a title="hepynet.train.train_utils.ray_tune" href="#hepynet.train.train_utils.ray_tune">ray_tune</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>