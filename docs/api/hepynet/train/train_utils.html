<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>hepynet.train.train_utils API documentation</title>
<meta name="description" content="Functions used for pDNN training â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>hepynet.train.train_utils</code></h1>
</header>
<section id="section-intro">
<p>Functions used for pDNN training.</p>
<p>This module is a collection of functions used for pDNN training. Include: array
manipulation, making plots, evaluation functions and so on.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;Functions used for pDNN training.

This module is a collection of functions used for pDNN training. Include: array
manipulation, making plots, evaluation functions and so on.

&#34;&#34;&#34;

import glob
import logging
import pathlib
import sys
import time
from math import sqrt

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score, auc, classification_report

from hepynet.common import array_utils, config_utils
from hepynet.data_io import numpy_io

logger = logging.getLogger(&#34;hepynet&#34;)


SEPA_KEYS = [
    &#34;xs_train&#34;,
    &#34;xs_test&#34;,
    &#34;ys_train&#34;,
    &#34;ys_test&#34;,
    &#34;wts_train&#34;,
    &#34;wts_test&#34;,
    &#34;xb_train&#34;,
    &#34;xb_test&#34;,
    &#34;yb_train&#34;,
    &#34;yb_test&#34;,
    &#34;wtb_train&#34;,
    &#34;wtb_test&#34;,
]

COMB_KEYS = [
    &#34;x_train&#34;,
    &#34;x_test&#34;,
    &#34;y_train&#34;,
    &#34;y_test&#34;,
    &#34;wt_train&#34;,
    &#34;wt_test&#34;,
]


def dump_fit_npy(
    feedbox, keras_model, fit_ntup_branches, output_bkg_node_names, npy_dir=&#34;./&#34;
):
    prefix_map = {&#34;sig&#34;: &#34;xs&#34;, &#34;bkg&#34;: &#34;xb&#34;}
    if feedbox.get_job_config().input.apply_data:
        prefix_map[&#34;data&#34;] = &#34;xd&#34;

    for map_key in list(prefix_map.keys()):
        sample_keys = getattr(feedbox.get_job_config().input, f&#34;{map_key}_list&#34;)
        for sample_key in sample_keys:
            dump_branches = fit_ntup_branches + [&#34;weight&#34;]
            # prepare contents
            dump_array, dump_array_weight = feedbox.get_raw(
                prefix_map[map_key], array_key=sample_key, add_validation_features=True,
            )
            predict_input, _ = feedbox.get_reweight(
                prefix_map[map_key], array_key=sample_key, reset_mass=False
            )
            predictions = keras_model.predict(predict_input)
            # dump
            platform_meta = config_utils.load_current_platform_meta()
            data_path = platform_meta[&#34;data_path&#34;]
            save_dir = f&#34;{data_path}/{npy_dir}&#34;
            pathlib.Path(save_dir).mkdir(parents=True, exist_ok=True)
            for branch in dump_branches:
                if branch == &#34;weight&#34;:
                    branch_content = dump_array_weight
                else:
                    fd_val_features = feedbox.get_job_config().input.validation_features
                    if fd_val_features is None:
                        validation_features = []
                    else:
                        validation_features = fd_val_features
                    feature_list = (
                        feedbox.get_job_config().input.selected_features
                        + validation_features
                    )
                    branch_index = feature_list.index(branch)
                    branch_content = dump_array[:, branch_index]
                save_path = f&#34;{save_dir}/{sample_key}_{branch}.npy&#34;
                numpy_io.save_npy_array(branch_content, save_path)
            if len(output_bkg_node_names) == 0:
                save_path = f&#34;{save_dir}/{sample_key}_dnn_out.npy&#34;
                numpy_io.save_npy_array(predictions, save_path)
            else:
                for i, out_node in enumerate([&#34;sig&#34;] + output_bkg_node_names):
                    out_node = out_node.replace(&#34;+&#34;, &#34;_&#34;)
                    save_path = f&#34;{save_dir}/{sample_key}_dnn_out_{out_node}.npy&#34;
                    numpy_io.save_npy_array(predictions[:, i], save_path)


def get_mass_range(mass_array, weights, nsig=1):
    &#34;&#34;&#34;Gives a range of mean +- sigma

    Note:
        Only use for single peak distribution

    &#34;&#34;&#34;
    average = np.average(mass_array, weights=weights)
    variance = np.average((mass_array - average) ** 2, weights=weights)
    lower_limit = average - np.sqrt(variance) * nsig
    upper_limit = average + np.sqrt(variance) * nsig
    return lower_limit, upper_limit


def get_model_epoch_path_list(
    load_dir, model_name, job_name=&#34;*&#34;, date=&#34;*&#34;, version=&#34;*&#34;
):
    # Search possible files
    search_pattern = load_dir + &#34;/&#34; + date + &#34;_&#34; + job_name + &#34;_&#34; + version + &#34;/models&#34;
    model_dir_list = glob.glob(search_pattern)
    # Choose the newest one
    if len(model_dir_list) &lt; 1:
        raise FileNotFoundError(&#34;Model file that matched the pattern not found.&#34;)
    model_dir = model_dir_list[-1]
    if len(model_dir_list) &gt; 1:
        logger.warning(
            &#34;More than one valid model file found, try to specify more infomation.&#34;
        )
        logger.info(f&#34;Loading the last matched model path: {model_dir}&#34;)
    else:
        logger.info(&#34;Loading model at: {model_dir}&#34;)
    search_pattern = model_dir + &#34;/&#34; + model_name + &#34;_epoch*.h5&#34;
    model_path_list = glob.glob(search_pattern)
    return model_path_list


def generate_shuffle_index(array_len, shuffle_seed=None):
    &#34;&#34;&#34;Generates array shuffle index.

    To use a consist shuffle index to have different arrays shuffle in same way.

    &#34;&#34;&#34;
    shuffle_index = np.array(range(array_len))
    if shuffle_seed is not None:
        np.random.seed(shuffle_seed)
    np.random.shuffle(shuffle_index)
    return shuffle_index


def get_mean_var(array, axis=None, weights=None):
    &#34;&#34;&#34;Calculate average and variance of an array.&#34;&#34;&#34;
    average = np.average(array, axis=axis, weights=weights)
    variance = np.average((array - average) ** 2, axis=axis, weights=weights)
    if 0 in variance:
        logger.warn(&#34;Encountered 0 variance, adding shift value 0.000001&#34;)
    return average, variance + 0.000001


def norarray(array, average=None, variance=None, axis=None, weights=None):
    &#34;&#34;&#34;Normalizes input array for each feature.

    Note:
        Do not normalize bkg and sig separately, bkg and sig should be normalized
        in the same way. (i.e. use same average and variance for normalization.)

    &#34;&#34;&#34;
    if len(array) == 0:
        return array
    else:
        if (average is None) or (variance is None):
            logger.warn(&#34;Unspecified average or variance.&#34;)
            average, variance = get_mean_var(array, axis=axis, weights=weights)
        output_array = (array.copy() - average) / np.sqrt(variance)
        return output_array


def norarray_min_max(array, min, max, axis=None):
    &#34;&#34;&#34;Normalizes input array to (-1, +1)&#34;&#34;&#34;
    middle = (min + max) / 2.0
    output_array = array.copy() - middle
    if max &lt; min:
        logger.error(&#34;ERROR: max shouldn&#39;t be smaller than min.&#34;)
        return None
    ratio = (max - min) / 2.0
    output_array = output_array / ratio


def split_and_combine(
    xs,
    xs_weight,
    xb,
    xb_weight,
    ys=None,
    yb=None,
    output_keys=None,
    test_rate=0.2,
    shuffle_combined_array=True,
    shuffle_seed=None,
):
    &#34;&#34;&#34;Prepares array for training &amp; validation

    Args:
        xs: numpy array
        Signal array for training.
        xb: numpy array
        Background array for training.
        test_rate: float, optional (default = 0.2)
        Portion of samples (array rows) to be used as independent test samples.
        shuffle_combined_array: bool, optional (default=True)
        Whether to shuffle outputs arrays before return.
        shuffle_seed: int or None, optional (default=None)
        Seed for randomization process.
        Set to None to use current time as seed.
        Set to a specific value to get an unchanged shuffle result.

    Returns:
        x_train/x_test/y_train/y_test: numpy array
        Array for training/testing.
        Contain mixed signal and background. 
        xs_test/xb_test: numpy array
        Array for scores plotting.
        Signal/background separated.

    &#34;&#34;&#34;
    # prepare
    if output_keys is None:
        output_keys = COMB_KEYS + SEPA_KEYS
    has_sepa = output_keys_has_sepa(output_keys)
    has_comb = output_keys_has_comb(output_keys)
    arr_sepa = DNN_Arrays_Separate()
    arr_comb = DNN_Arrays_Combined()
    if ys is None:
        ys = np.ones(len(xs)).reshape(-1, 1)
    if yb is None:
        yb = np.zeros(len(xb)).reshape(-1, 1)

    (
        arr_sepa.xs_train,
        arr_sepa.xs_test,
        arr_sepa.ys_train,
        arr_sepa.ys_test,
        arr_sepa.wts_train,
        arr_sepa.wts_test,
    ) = array_utils.shuffle_and_split(
        xs, ys, xs_weight, split_ratio=1 - test_rate, shuffle_seed=shuffle_seed
    )
    (
        arr_sepa.xb_train,
        arr_sepa.xb_test,
        arr_sepa.yb_train,
        arr_sepa.yb_test,
        arr_sepa.wtb_train,
        arr_sepa.wtb_test,
    ) = array_utils.shuffle_and_split(
        xb, yb, xb_weight, split_ratio=1 - test_rate, shuffle_seed=shuffle_seed
    )

    if has_comb:
        arr_comb.x_train = np.concatenate((arr_sepa.xs_train, arr_sepa.xb_train))
        arr_comb.y_train = np.concatenate((arr_sepa.ys_train, arr_sepa.yb_train))
        arr_comb.wt_train = np.concatenate((arr_sepa.wts_train, arr_sepa.wtb_train))
        arr_comb.x_test = np.concatenate((arr_sepa.xs_test, arr_sepa.xb_test))
        arr_comb.y_test = np.concatenate((arr_sepa.ys_test, arr_sepa.yb_test))
        arr_comb.wt_test = np.concatenate((arr_sepa.wts_test, arr_sepa.wtb_test))
        if not has_sepa:
            arr_sepa = None
        if shuffle_combined_array:
            # shuffle train dataset
            shuffle_index = generate_shuffle_index(
                len(arr_comb.y_train), shuffle_seed=shuffle_seed
            )
            arr_comb.x_train = arr_comb.x_train[shuffle_index]
            arr_comb.y_train = arr_comb.y_train[shuffle_index]
            arr_comb.wt_train = arr_comb.wt_train[shuffle_index]
            # shuffle test dataset
            shuffle_index = generate_shuffle_index(
                len(arr_comb.y_test), shuffle_seed=shuffle_seed
            )
            arr_comb.x_test = arr_comb.x_test[shuffle_index]
            arr_comb.y_test = arr_comb.y_test[shuffle_index]
            arr_comb.wt_test = arr_comb.wt_test[shuffle_index]

    out_arrays = {}
    for key in output_keys:
        if key in SEPA_KEYS:
            out_arrays[key] = getattr(arr_sepa, key)
        elif key in COMB_KEYS:
            out_arrays[key] = getattr(arr_comb, key)
        else:
            logger.error(f&#34;Unknown output_key: {key}&#34;)

    return out_arrays


def output_keys_has_sepa(output_keys):
    for key in output_keys:
        if key in SEPA_KEYS:
            return True
    return False


def output_keys_has_comb(output_keys):
    for key in output_keys:
        if key in COMB_KEYS:
            return True
    return False


class DNN_Arrays_Separate(object):
    def __init__(self):
        self.xs_train = None
        self.xs_test = None
        self.ys_train = None
        self.ys_test = None
        self.wts_train = None
        self.wts_test = None
        self.xb_train = None
        self.xb_test = None
        self.yb_train = None
        self.yb_test = None
        self.wtb_train = None
        self.wtb_test = None


class DNN_Arrays_Combined(object):
    def __init__(self):
        self.x_train = None
        self.x_test = None
        self.y_train = None
        self.y_test = None
        self.wt_train = None
        self.wt_test = None</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="hepynet.train.train_utils.dump_fit_npy"><code class="name flex">
<span>def <span class="ident">dump_fit_npy</span></span>(<span>feedbox, keras_model, fit_ntup_branches, output_bkg_node_names, npy_dir='./')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump_fit_npy(
    feedbox, keras_model, fit_ntup_branches, output_bkg_node_names, npy_dir=&#34;./&#34;
):
    prefix_map = {&#34;sig&#34;: &#34;xs&#34;, &#34;bkg&#34;: &#34;xb&#34;}
    if feedbox.get_job_config().input.apply_data:
        prefix_map[&#34;data&#34;] = &#34;xd&#34;

    for map_key in list(prefix_map.keys()):
        sample_keys = getattr(feedbox.get_job_config().input, f&#34;{map_key}_list&#34;)
        for sample_key in sample_keys:
            dump_branches = fit_ntup_branches + [&#34;weight&#34;]
            # prepare contents
            dump_array, dump_array_weight = feedbox.get_raw(
                prefix_map[map_key], array_key=sample_key, add_validation_features=True,
            )
            predict_input, _ = feedbox.get_reweight(
                prefix_map[map_key], array_key=sample_key, reset_mass=False
            )
            predictions = keras_model.predict(predict_input)
            # dump
            platform_meta = config_utils.load_current_platform_meta()
            data_path = platform_meta[&#34;data_path&#34;]
            save_dir = f&#34;{data_path}/{npy_dir}&#34;
            pathlib.Path(save_dir).mkdir(parents=True, exist_ok=True)
            for branch in dump_branches:
                if branch == &#34;weight&#34;:
                    branch_content = dump_array_weight
                else:
                    fd_val_features = feedbox.get_job_config().input.validation_features
                    if fd_val_features is None:
                        validation_features = []
                    else:
                        validation_features = fd_val_features
                    feature_list = (
                        feedbox.get_job_config().input.selected_features
                        + validation_features
                    )
                    branch_index = feature_list.index(branch)
                    branch_content = dump_array[:, branch_index]
                save_path = f&#34;{save_dir}/{sample_key}_{branch}.npy&#34;
                numpy_io.save_npy_array(branch_content, save_path)
            if len(output_bkg_node_names) == 0:
                save_path = f&#34;{save_dir}/{sample_key}_dnn_out.npy&#34;
                numpy_io.save_npy_array(predictions, save_path)
            else:
                for i, out_node in enumerate([&#34;sig&#34;] + output_bkg_node_names):
                    out_node = out_node.replace(&#34;+&#34;, &#34;_&#34;)
                    save_path = f&#34;{save_dir}/{sample_key}_dnn_out_{out_node}.npy&#34;
                    numpy_io.save_npy_array(predictions[:, i], save_path)</code></pre>
</details>
</dd>
<dt id="hepynet.train.train_utils.generate_shuffle_index"><code class="name flex">
<span>def <span class="ident">generate_shuffle_index</span></span>(<span>array_len, shuffle_seed=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates array shuffle index.</p>
<p>To use a consist shuffle index to have different arrays shuffle in same way.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_shuffle_index(array_len, shuffle_seed=None):
    &#34;&#34;&#34;Generates array shuffle index.

    To use a consist shuffle index to have different arrays shuffle in same way.

    &#34;&#34;&#34;
    shuffle_index = np.array(range(array_len))
    if shuffle_seed is not None:
        np.random.seed(shuffle_seed)
    np.random.shuffle(shuffle_index)
    return shuffle_index</code></pre>
</details>
</dd>
<dt id="hepynet.train.train_utils.get_mass_range"><code class="name flex">
<span>def <span class="ident">get_mass_range</span></span>(<span>mass_array, weights, nsig=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Gives a range of mean +- sigma</p>
<h2 id="note">Note</h2>
<p>Only use for single peak distribution</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_mass_range(mass_array, weights, nsig=1):
    &#34;&#34;&#34;Gives a range of mean +- sigma

    Note:
        Only use for single peak distribution

    &#34;&#34;&#34;
    average = np.average(mass_array, weights=weights)
    variance = np.average((mass_array - average) ** 2, weights=weights)
    lower_limit = average - np.sqrt(variance) * nsig
    upper_limit = average + np.sqrt(variance) * nsig
    return lower_limit, upper_limit</code></pre>
</details>
</dd>
<dt id="hepynet.train.train_utils.get_mean_var"><code class="name flex">
<span>def <span class="ident">get_mean_var</span></span>(<span>array, axis=None, weights=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate average and variance of an array.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_mean_var(array, axis=None, weights=None):
    &#34;&#34;&#34;Calculate average and variance of an array.&#34;&#34;&#34;
    average = np.average(array, axis=axis, weights=weights)
    variance = np.average((array - average) ** 2, axis=axis, weights=weights)
    if 0 in variance:
        logger.warn(&#34;Encountered 0 variance, adding shift value 0.000001&#34;)
    return average, variance + 0.000001</code></pre>
</details>
</dd>
<dt id="hepynet.train.train_utils.get_model_epoch_path_list"><code class="name flex">
<span>def <span class="ident">get_model_epoch_path_list</span></span>(<span>load_dir, model_name, job_name='*', date='*', version='*')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_model_epoch_path_list(
    load_dir, model_name, job_name=&#34;*&#34;, date=&#34;*&#34;, version=&#34;*&#34;
):
    # Search possible files
    search_pattern = load_dir + &#34;/&#34; + date + &#34;_&#34; + job_name + &#34;_&#34; + version + &#34;/models&#34;
    model_dir_list = glob.glob(search_pattern)
    # Choose the newest one
    if len(model_dir_list) &lt; 1:
        raise FileNotFoundError(&#34;Model file that matched the pattern not found.&#34;)
    model_dir = model_dir_list[-1]
    if len(model_dir_list) &gt; 1:
        logger.warning(
            &#34;More than one valid model file found, try to specify more infomation.&#34;
        )
        logger.info(f&#34;Loading the last matched model path: {model_dir}&#34;)
    else:
        logger.info(&#34;Loading model at: {model_dir}&#34;)
    search_pattern = model_dir + &#34;/&#34; + model_name + &#34;_epoch*.h5&#34;
    model_path_list = glob.glob(search_pattern)
    return model_path_list</code></pre>
</details>
</dd>
<dt id="hepynet.train.train_utils.norarray"><code class="name flex">
<span>def <span class="ident">norarray</span></span>(<span>array, average=None, variance=None, axis=None, weights=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Normalizes input array for each feature.</p>
<h2 id="note">Note</h2>
<p>Do not normalize bkg and sig separately, bkg and sig should be normalized
in the same way. (i.e. use same average and variance for normalization.)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def norarray(array, average=None, variance=None, axis=None, weights=None):
    &#34;&#34;&#34;Normalizes input array for each feature.

    Note:
        Do not normalize bkg and sig separately, bkg and sig should be normalized
        in the same way. (i.e. use same average and variance for normalization.)

    &#34;&#34;&#34;
    if len(array) == 0:
        return array
    else:
        if (average is None) or (variance is None):
            logger.warn(&#34;Unspecified average or variance.&#34;)
            average, variance = get_mean_var(array, axis=axis, weights=weights)
        output_array = (array.copy() - average) / np.sqrt(variance)
        return output_array</code></pre>
</details>
</dd>
<dt id="hepynet.train.train_utils.norarray_min_max"><code class="name flex">
<span>def <span class="ident">norarray_min_max</span></span>(<span>array, min, max, axis=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Normalizes input array to (-1, +1)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def norarray_min_max(array, min, max, axis=None):
    &#34;&#34;&#34;Normalizes input array to (-1, +1)&#34;&#34;&#34;
    middle = (min + max) / 2.0
    output_array = array.copy() - middle
    if max &lt; min:
        logger.error(&#34;ERROR: max shouldn&#39;t be smaller than min.&#34;)
        return None
    ratio = (max - min) / 2.0
    output_array = output_array / ratio</code></pre>
</details>
</dd>
<dt id="hepynet.train.train_utils.output_keys_has_comb"><code class="name flex">
<span>def <span class="ident">output_keys_has_comb</span></span>(<span>output_keys)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def output_keys_has_comb(output_keys):
    for key in output_keys:
        if key in COMB_KEYS:
            return True
    return False</code></pre>
</details>
</dd>
<dt id="hepynet.train.train_utils.output_keys_has_sepa"><code class="name flex">
<span>def <span class="ident">output_keys_has_sepa</span></span>(<span>output_keys)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def output_keys_has_sepa(output_keys):
    for key in output_keys:
        if key in SEPA_KEYS:
            return True
    return False</code></pre>
</details>
</dd>
<dt id="hepynet.train.train_utils.split_and_combine"><code class="name flex">
<span>def <span class="ident">split_and_combine</span></span>(<span>xs, xs_weight, xb, xb_weight, ys=None, yb=None, output_keys=None, test_rate=0.2, shuffle_combined_array=True, shuffle_seed=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Prepares array for training &amp; validation</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>xs</code></strong></dt>
<dd>numpy array</dd>
<dt>Signal array for training.</dt>
<dt><strong><code>xb</code></strong></dt>
<dd>numpy array</dd>
<dt>Background array for training.</dt>
<dt><strong><code>test_rate</code></strong></dt>
<dd>float, optional (default = 0.2)</dd>
<dt>Portion of samples (array rows) to be used as independent test samples.</dt>
<dt><strong><code>shuffle_combined_array</code></strong></dt>
<dd>bool, optional (default=True)</dd>
<dt>Whether to shuffle outputs arrays before return.</dt>
<dt><strong><code>shuffle_seed</code></strong></dt>
<dd>int or None, optional (default=None)</dd>
</dl>
<p>Seed for randomization process.
Set to None to use current time as seed.
Set to a specific value to get an unchanged shuffle result.</p>
<h2 id="returns">Returns</h2>
<p>x_train/x_test/y_train/y_test: numpy array
Array for training/testing.
Contain mixed signal and background.
xs_test/xb_test: numpy array
Array for scores plotting.
Signal/background separated.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def split_and_combine(
    xs,
    xs_weight,
    xb,
    xb_weight,
    ys=None,
    yb=None,
    output_keys=None,
    test_rate=0.2,
    shuffle_combined_array=True,
    shuffle_seed=None,
):
    &#34;&#34;&#34;Prepares array for training &amp; validation

    Args:
        xs: numpy array
        Signal array for training.
        xb: numpy array
        Background array for training.
        test_rate: float, optional (default = 0.2)
        Portion of samples (array rows) to be used as independent test samples.
        shuffle_combined_array: bool, optional (default=True)
        Whether to shuffle outputs arrays before return.
        shuffle_seed: int or None, optional (default=None)
        Seed for randomization process.
        Set to None to use current time as seed.
        Set to a specific value to get an unchanged shuffle result.

    Returns:
        x_train/x_test/y_train/y_test: numpy array
        Array for training/testing.
        Contain mixed signal and background. 
        xs_test/xb_test: numpy array
        Array for scores plotting.
        Signal/background separated.

    &#34;&#34;&#34;
    # prepare
    if output_keys is None:
        output_keys = COMB_KEYS + SEPA_KEYS
    has_sepa = output_keys_has_sepa(output_keys)
    has_comb = output_keys_has_comb(output_keys)
    arr_sepa = DNN_Arrays_Separate()
    arr_comb = DNN_Arrays_Combined()
    if ys is None:
        ys = np.ones(len(xs)).reshape(-1, 1)
    if yb is None:
        yb = np.zeros(len(xb)).reshape(-1, 1)

    (
        arr_sepa.xs_train,
        arr_sepa.xs_test,
        arr_sepa.ys_train,
        arr_sepa.ys_test,
        arr_sepa.wts_train,
        arr_sepa.wts_test,
    ) = array_utils.shuffle_and_split(
        xs, ys, xs_weight, split_ratio=1 - test_rate, shuffle_seed=shuffle_seed
    )
    (
        arr_sepa.xb_train,
        arr_sepa.xb_test,
        arr_sepa.yb_train,
        arr_sepa.yb_test,
        arr_sepa.wtb_train,
        arr_sepa.wtb_test,
    ) = array_utils.shuffle_and_split(
        xb, yb, xb_weight, split_ratio=1 - test_rate, shuffle_seed=shuffle_seed
    )

    if has_comb:
        arr_comb.x_train = np.concatenate((arr_sepa.xs_train, arr_sepa.xb_train))
        arr_comb.y_train = np.concatenate((arr_sepa.ys_train, arr_sepa.yb_train))
        arr_comb.wt_train = np.concatenate((arr_sepa.wts_train, arr_sepa.wtb_train))
        arr_comb.x_test = np.concatenate((arr_sepa.xs_test, arr_sepa.xb_test))
        arr_comb.y_test = np.concatenate((arr_sepa.ys_test, arr_sepa.yb_test))
        arr_comb.wt_test = np.concatenate((arr_sepa.wts_test, arr_sepa.wtb_test))
        if not has_sepa:
            arr_sepa = None
        if shuffle_combined_array:
            # shuffle train dataset
            shuffle_index = generate_shuffle_index(
                len(arr_comb.y_train), shuffle_seed=shuffle_seed
            )
            arr_comb.x_train = arr_comb.x_train[shuffle_index]
            arr_comb.y_train = arr_comb.y_train[shuffle_index]
            arr_comb.wt_train = arr_comb.wt_train[shuffle_index]
            # shuffle test dataset
            shuffle_index = generate_shuffle_index(
                len(arr_comb.y_test), shuffle_seed=shuffle_seed
            )
            arr_comb.x_test = arr_comb.x_test[shuffle_index]
            arr_comb.y_test = arr_comb.y_test[shuffle_index]
            arr_comb.wt_test = arr_comb.wt_test[shuffle_index]

    out_arrays = {}
    for key in output_keys:
        if key in SEPA_KEYS:
            out_arrays[key] = getattr(arr_sepa, key)
        elif key in COMB_KEYS:
            out_arrays[key] = getattr(arr_comb, key)
        else:
            logger.error(f&#34;Unknown output_key: {key}&#34;)

    return out_arrays</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="hepynet.train.train_utils.DNN_Arrays_Combined"><code class="flex name class">
<span>class <span class="ident">DNN_Arrays_Combined</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DNN_Arrays_Combined(object):
    def __init__(self):
        self.x_train = None
        self.x_test = None
        self.y_train = None
        self.y_test = None
        self.wt_train = None
        self.wt_test = None</code></pre>
</details>
</dd>
<dt id="hepynet.train.train_utils.DNN_Arrays_Separate"><code class="flex name class">
<span>class <span class="ident">DNN_Arrays_Separate</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DNN_Arrays_Separate(object):
    def __init__(self):
        self.xs_train = None
        self.xs_test = None
        self.ys_train = None
        self.ys_test = None
        self.wts_train = None
        self.wts_test = None
        self.xb_train = None
        self.xb_test = None
        self.yb_train = None
        self.yb_test = None
        self.wtb_train = None
        self.wtb_test = None</code></pre>
</details>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="hepynet.train" href="index.html">hepynet.train</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="hepynet.train.train_utils.dump_fit_npy" href="#hepynet.train.train_utils.dump_fit_npy">dump_fit_npy</a></code></li>
<li><code><a title="hepynet.train.train_utils.generate_shuffle_index" href="#hepynet.train.train_utils.generate_shuffle_index">generate_shuffle_index</a></code></li>
<li><code><a title="hepynet.train.train_utils.get_mass_range" href="#hepynet.train.train_utils.get_mass_range">get_mass_range</a></code></li>
<li><code><a title="hepynet.train.train_utils.get_mean_var" href="#hepynet.train.train_utils.get_mean_var">get_mean_var</a></code></li>
<li><code><a title="hepynet.train.train_utils.get_model_epoch_path_list" href="#hepynet.train.train_utils.get_model_epoch_path_list">get_model_epoch_path_list</a></code></li>
<li><code><a title="hepynet.train.train_utils.norarray" href="#hepynet.train.train_utils.norarray">norarray</a></code></li>
<li><code><a title="hepynet.train.train_utils.norarray_min_max" href="#hepynet.train.train_utils.norarray_min_max">norarray_min_max</a></code></li>
<li><code><a title="hepynet.train.train_utils.output_keys_has_comb" href="#hepynet.train.train_utils.output_keys_has_comb">output_keys_has_comb</a></code></li>
<li><code><a title="hepynet.train.train_utils.output_keys_has_sepa" href="#hepynet.train.train_utils.output_keys_has_sepa">output_keys_has_sepa</a></code></li>
<li><code><a title="hepynet.train.train_utils.split_and_combine" href="#hepynet.train.train_utils.split_and_combine">split_and_combine</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="hepynet.train.train_utils.DNN_Arrays_Combined" href="#hepynet.train.train_utils.DNN_Arrays_Combined">DNN_Arrays_Combined</a></code></h4>
</li>
<li>
<h4><code><a title="hepynet.train.train_utils.DNN_Arrays_Separate" href="#hepynet.train.train_utils.DNN_Arrays_Separate">DNN_Arrays_Separate</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>