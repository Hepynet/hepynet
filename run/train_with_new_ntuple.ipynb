{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with new ntuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.18/00\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # add self-defined module in the parent path\n",
    "sys.path.append(\"../..\") # add self-defined module in the parent path\n",
    "import time\n",
    "\n",
    "from array import array\n",
    "import datetime\n",
    "import keras.backend\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Concatenate, Dense, Input\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adagrad, SGD, RMSprop, Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc\n",
    "from sklearn import preprocessing\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from lfv_pdnn_code_v1.train import model\n",
    "from lfv_pdnn_code_v1.train.train_utils import *\n",
    "from lfv_pdnn_code_v1.common.common_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load new array\n",
    "### a) load background samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading new background array.\n",
      "xb_di_boson shape: (427882, 24)\n",
      "xb_top_quark shape: (1122168, 24)\n",
      "xb_w_jets shape: (2015589, 24)\n",
      "xb_z_ll shape: (128656, 24)\n",
      "\n",
      "Adding all background together.\n",
      "xb shape: (3694295, 24)\n",
      "\n",
      "Organizing new background with dict: xb_dict_new.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Set parent path to access data\n",
    "data_path = \"/mnt/e/data/new_ntuple/mc16a\"\n",
    "\n",
    "# Load\n",
    "print \"Loading new background array.\"\n",
    "# di_boson\n",
    "directory = data_path + \"/di_boson\"\n",
    "search_pattern = \"*.npy\"\n",
    "absolute_file_list, file_name_list = get_file_list(directory, search_pattern)\n",
    "xb_di_boson = np.array([])\n",
    "for path in absolute_file_list:\n",
    "    temp_array = np.load(path)\n",
    "    if len(temp_array) == 0:\n",
    "        continue\n",
    "    if len(xb_di_boson) == 0:\n",
    "        xb_di_boson = temp_array.copy()\n",
    "    else:\n",
    "        xb_di_boson = np.concatenate((xb_di_boson, temp_array))\n",
    "print \"xb_di_boson shape:\", xb_di_boson.shape\n",
    "# top_quark\n",
    "directory = data_path + \"/top_quark\"\n",
    "search_pattern = \"*.npy\"\n",
    "absolute_file_list, file_name_list = get_file_list(directory, search_pattern)\n",
    "xb_top_quark = np.array([])\n",
    "for path in absolute_file_list:\n",
    "    temp_array = np.load(path)\n",
    "    if len(temp_array) == 0:\n",
    "        continue\n",
    "    if len(xb_top_quark) == 0:\n",
    "        xb_top_quark = temp_array.copy()\n",
    "    else:\n",
    "        xb_top_quark = np.concatenate((xb_top_quark, temp_array))\n",
    "print \"xb_top_quark shape:\", xb_top_quark.shape\n",
    "# w_jets\n",
    "directory = data_path + \"/w_jets\"\n",
    "search_pattern = \"*.npy\"\n",
    "absolute_file_list, file_name_list = get_file_list(directory, search_pattern)\n",
    "xb_w_jets = np.array([])\n",
    "for path in absolute_file_list:\n",
    "    temp_array = np.load(path)\n",
    "    if len(temp_array) == 0:\n",
    "        continue\n",
    "    if len(xb_w_jets) == 0:\n",
    "        xb_w_jets = temp_array.copy()\n",
    "    else:\n",
    "        xb_w_jets = np.concatenate((xb_w_jets, temp_array))\n",
    "print \"xb_w_jets shape:\", xb_w_jets.shape\n",
    "# z_ll\n",
    "directory = data_path + \"/z_ll\"\n",
    "search_pattern = \"*.npy\"\n",
    "absolute_file_list, file_name_list = get_file_list(directory, search_pattern)\n",
    "xb_z_ll = np.array([])\n",
    "for path in absolute_file_list:\n",
    "    temp_array = np.load(path)\n",
    "    if len(temp_array) == 0:\n",
    "        continue\n",
    "    if len(xb_z_ll) == 0:\n",
    "        xb_z_ll = temp_array.copy()\n",
    "    else:\n",
    "        xb_z_ll = np.concatenate((xb_z_ll, temp_array))\n",
    "print \"xb_z_ll shape:\", xb_z_ll.shape\n",
    "\n",
    "# Add all background together\n",
    "print \"\\nAdding all background together.\"\n",
    "xb = np.concatenate((xb_di_boson, xb_top_quark, xb_w_jets, xb_z_ll))\n",
    "print \"xb shape:\", xb.shape\n",
    "\n",
    "# Organize with dict\n",
    "print \"\\nOrganizing new background with dict: xb_dict_new.\"\n",
    "xb_dict_new = {}\n",
    "xb_dict_new['di_boson'] = xb_di_boson\n",
    "xb_dict_new['top_quark'] = xb_top_quark\n",
    "xb_dict_new['w_jets'] = xb_w_jets\n",
    "xb_dict_new['z_ll'] = xb_z_ll\n",
    "xb_dict_new['all'] = xb\n",
    "\n",
    "print \"\\nDone.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) load signal samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading new signal array.\n",
      "\n",
      "Organizing new signal with dict: xs_dict_new.\n",
      "adding 500GeV signal to xs_dict_new\n",
      "adding 2000GeV signal to xs_dict_new\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "mass_min = 5000\n",
    "mass_max = 0\n",
    "xs_studied = np.array([])\n",
    "mass_scan_map = [500, 2000]\n",
    "#mass_scan_map = [500]\n",
    "xs_dict_new = {}\n",
    "\n",
    "# Load\n",
    "print \"Loading new signal array.\"\n",
    "print \"\\nOrganizing new signal with dict: xs_dict_new.\"\n",
    "data_path = \"/mnt/e/data/new_ntuple/mc16a\"\n",
    "xs = np.array([])\n",
    "for i, mass in enumerate(mass_scan_map):\n",
    "    # load signal\n",
    "    xs_add = np.load(data_path + \"/signal/rpv_emu_{}GeV.npy\".format(mass))\n",
    "    xs_temp = np.load(data_path + \"/signal/rpv_etau_{}GeV.npy\".format(mass))\n",
    "    xs_add = np.concatenate((xs_add, xs_temp))\n",
    "    xs_temp = np.load(data_path + \"/signal/rpv_mutau_{}GeV.npy\".format(mass))\n",
    "    xs_add = np.concatenate((xs_add, xs_temp))\n",
    "    # add to dict xs_dict_new\n",
    "    print \"adding {}GeV signal to xs_dict_new\".format(mass)\n",
    "    xs_dict_new['{}GeV'.format(mass)] = xs_add\n",
    "    # add to full signals\n",
    "    if len(xs) == 0:\n",
    "        xs = xs_add.copy()\n",
    "    else:\n",
    "        xs = np.concatenate((xs, xs_add))\n",
    "xs_dict_new['all'] = xs\n",
    "\n",
    "print \"\\nDone.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load old array\n",
    "### a) load old background samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading new background array.\n",
      "\n",
      "Organizing new background with dict: xb_dict_old.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Load\n",
    "print \"Loading new background array.\"\n",
    "xb_di_boson_old = np.load('/mnt/e/data/lfv/ntuples_last_run/TestData/data_npy/tree_bkg1.npy')\n",
    "xb_drell_yan_old = np.load('/mnt/e/data/lfv/ntuples_last_run/TestData/data_npy/tree_bkg2.npy')\n",
    "xb_top_quark_old = np.load('/mnt/e/data/lfv/ntuples_last_run/TestData/data_npy/tree_bkg3.npy')\n",
    "xb_w_jets_old = np.load('/mnt/e/data/lfv/ntuples_last_run/TestData/data_npy/tree_bkg4.npy')\n",
    "xb_z_ll_old = np.load('/mnt/e/data/lfv/ntuples_last_run/TestData/data_npy/tree_bkg5.npy')\n",
    "xb_old = np.concatenate((xb_di_boson_old, xb_drell_yan_old, xb_top_quark_old, xb_w_jets_old, xb_z_ll_old))\n",
    "\n",
    "# Organize with dict\n",
    "print \"\\nOrganizing new background with dict: xb_dict_old.\"\n",
    "xb_dict_old = {}\n",
    "xb_dict_old['di_boson'] = xb_di_boson_old\n",
    "xb_dict_old['drell_yan'] = xb_drell_yan_old\n",
    "xb_dict_old['top_quark'] = xb_top_quark_old\n",
    "xb_dict_old['w_jets'] = xb_w_jets_old\n",
    "xb_dict_old['z_ll'] = xb_z_ll_old\n",
    "xb_dict_old['all'] = xb_old\n",
    "\n",
    "print \"\\nDone.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) load old signal samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading old signal array.\n",
      "\n",
      "Organizing old signal with dict: xs_dict_old.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "mass_min = 5000\n",
    "mass_max = 0\n",
    "xs_old = np.array([])\n",
    "xs_dict_old = {}\n",
    "mass_scan_map = [5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,22,24,26,28,30,35,40,45,50]\n",
    "\n",
    "# Load\n",
    "print \"Loading old signal array.\"\n",
    "print \"\\nOrganizing old signal with dict: xs_dict_old.\"\n",
    "for i, mass in enumerate(mass_scan_map):\n",
    "    # load signal\n",
    "    xs_add = np.load('/mnt/e/data/lfv/ntuples_last_run/train_array_0909/data_npy/emu/rpv_{}00GeV.npy'.format(mass))\n",
    "    xs_temp = np.load('/mnt/e/data/lfv/ntuples_last_run/train_array_0909/data_npy/etau/rpv_{}00GeV.npy'.format(mass))\n",
    "    xs_add = np.concatenate((xs_add, xs_temp))\n",
    "    xs_temp = np.load('/mnt/e/data/lfv/ntuples_last_run/train_array_0909/data_npy/mutau/rpv_{}00GeV.npy'.format(mass))\n",
    "    xs_add = np.concatenate((xs_add, xs_temp))\n",
    "    # add to dict xs_dict_new\n",
    "    #print \"adding {}00GeV signal to xs_dict_old\".format(mass)\n",
    "    xs_dict_old['{}00GeV'.format(mass)] = xs_add\n",
    "    # add to full signals\n",
    "    if len(xs_old) == 0:\n",
    "        xs_old = xs_add.copy()\n",
    "    else:\n",
    "        xs_old = np.concatenate((xs_old, xs_add))\n",
    "xs_dict_old['all'] = xs\n",
    "\n",
    "print \"\\nDone.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - kinematic plots for emu channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MakePlots(xs_studied, xb, 0, bins = 50, range = (0, 3000)  , density = True,\n",
    "          xlabel = \"di-Lepton mass / GeV\", ylabel = \"events\", show_plot = True)\n",
    "\n",
    "MakePlots(xs_emu, xb_emu, 1, bins = 50, range = (0, 1000)  , density = True,\n",
    "          xlabel=\"Ele_pt / GeV\"          , ylabel=\"Events\"  , show_plot = True)\n",
    "MakePlots(xs_emu, xb_emu, 2, bins = 50, range = (-3, 3)    , density = True,\n",
    "          xlabel=\"Ele_eta\"               , ylabel=\"Events\"  , show_plot = True)\n",
    "MakePlots(xs_emu, xb_emu, 3, bins = 50, range = (-3.2, 3.2), density = True,\n",
    "          xlabel=\"Ele_phi\"               , ylabel=\"Events\"  , show_plot = True)\n",
    "MakePlots(xs_emu, xb_emu, 5, bins = 50, range = (0, 5000)  , density = True,\n",
    "          xlabel=\"Mu_pt / GeV\"           , ylabel=\"Events\"  , show_plot = True)\n",
    "MakePlots(xs_emu, xb_emu, 6, bins = 50, range = (-3, 3)    , density = True,\n",
    "          xlabel=\"Mu_eta\"                , ylabel=\"Events\"  , show_plot = True)\n",
    "MakePlots(xs_emu, xb_emu, 7, bins = 50, range = (-3.2, 3.2), density = True,\n",
    "          xlabel=\"Mu_phi\"                , ylabel=\"Events\"  , show_plot = True)\n",
    "\n",
    "MakePlots(xs_emu, xb_emu, 15, bins = 50, range = (0, 1000)  , density = True,\n",
    "          xlabel=\"di-Lepton_pt\"          , ylabel=\"Events\"  , show_plot = True)\n",
    "MakePlots(xs_emu, xb_emu, 16, bins = 50, range = (-3, 3)    , density = True,\n",
    "          xlabel=\"di-Lepton_eta\"         , ylabel=\"Events\"  , show_plot = True)\n",
    "MakePlots(xs_emu, xb_emu, 17, bins = 50, range = (-3.2, 3.2), density = True,\n",
    "          xlabel=\"di-Lepton_phi\"         , ylabel=\"Events\"  , show_plot = True)\n",
    "MakePlots(xs_emu, xb_emu, 18, bins = 50, range = (-3.2, 3.2), density = True,\n",
    "          xlabel=\"di-Lepton_Dphi\"        , ylabel=\"Events\"  , show_plot = True)\n",
    "MakePlots(xs_emu, xb_emu, 18, bins = 50, range = (0, 5)     , density = True,\n",
    "          xlabel=\"di-Lepton_DR\"          , ylabel=\"Events\"  , show_plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Train with individual mass point (new vs old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) mass = 500 GeV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* old ntuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training emu.\n",
      "Loading signal.\n",
      "mass range: 472.9450298287913 to 527.0549701712088\n",
      "Loading background.\n",
      "Is model comiled? False\n",
      "[debug]compile\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../lfv_pdnn_code_v1/train/model.py:41: UserWarning: model is not compiled\n",
      "  warnings.warn(\"model is not compiled\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sig events quantity: 8284\n",
      "bkg events quantity: 5912\n",
      "train data shape: (11356, 12)\n",
      "test data shape: (2840, 12)\n",
      "Train on 8517 samples, validate on 2839 samples\n",
      "Epoch 1/30\n",
      "8517/8517 [==============================] - 4s 453us/step - loss: 0.0846 - acc: 0.7060 - val_loss: 0.1425 - val_acc: 0.5470\n",
      "Epoch 2/30\n",
      "8517/8517 [==============================] - 2s 203us/step - loss: 0.0748 - acc: 0.7422 - val_loss: 0.1038 - val_acc: 0.7348\n",
      "Epoch 3/30\n",
      "7400/8517 [=========================>....] - ETA: 0s - loss: 0.0698 - acc: 0.7557"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2fd2ed27bbb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m train_history = model_deep.get_model().fit(x_emu_train_selected, y_emu_train, batch_size = 100, epochs = 30,\n\u001b[0;32m---> 66\u001b[0;31m                 validation_split = 0.25 , sample_weight = x_emu_train[:, -1], verbose = 1)\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_deep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_emu_test_selected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_emu_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_emu_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zheya/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/zheya/.local/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zheya/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zheya/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zheya/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from lfv_pdnn_code_v1.train import model\n",
    "reload(model)\n",
    "reload(print_helper)\n",
    "####\n",
    "\n",
    "selected_features = [0, 1, 2, 3, 5, 6, 7, 15, 16, 17, 18, 19]\n",
    "\n",
    "# emu\n",
    "print \"Training emu.\"\n",
    "# get emu\n",
    "print \"Loading signal.\"\n",
    "xs_emu = modify_array(xs_dict_old['500GeV'], weight_id = -1, select_channel = True, channel_id = -4,\n",
    "                      norm = True, shuffle = True, shuffle_seed = int(time.time()))\n",
    "mass_sigma = get_mass_range(xs_emu[:, 0], xs_emu[:, -1])\n",
    "mass_min = 500 - mass_sigma\n",
    "mass_max = 500 + mass_sigma\n",
    "print \"mass range:\", mass_min, \"to\", mass_max\n",
    "print \"Loading background.\"\n",
    "xb_emu = modify_array(xb_dict_old['all'], weight_id = -1, remove_negative_weight = True, select_channel = True, channel_id = -4,\n",
    "                      select_mass = True, mass_id = 0, mass_min = mass_min, mass_max = mass_max,\n",
    "                      reset_mass = True, reset_mass_array = xs_emu, reset_mass_id = 0,\n",
    "                      norm = True, shuffle = True)\n",
    "# set model\n",
    "model_deep = model.model_0913(\"model_mass_500\", len(selected_features))\n",
    "model_deep.get_model()\n",
    "print \"[debug]compile\"\n",
    "model_deep.compile()\n",
    "\"\"\"\n",
    "node_num = 300\n",
    "model_deep = Sequential()\n",
    "model_deep.add(Dense(node_num, kernel_initializer='uniform', input_dim = len(selected_features)))\n",
    "model_deep.add(BatchNormalization())\n",
    "model_deep.add(Dense(node_num, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "model_deep.add(BatchNormalization())\n",
    "model_deep.add(Dense(node_num, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "model_deep.add(BatchNormalization())\n",
    "model_deep.add(Dense(node_num, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "model_deep.add(BatchNormalization())\n",
    "model_deep.add(Dense(node_num, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "model_deep.add(BatchNormalization())\n",
    "model_deep.add(Dense(node_num, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "model_deep.add(BatchNormalization())\n",
    "model_deep.add(Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "# set loss, optimizer and evaluation metrics\n",
    "model_deep.compile(loss=\"binary_crossentropy\", optimizer=SGD(lr=0.025, decay=1e-6), metrics=[\"accuracy\"])\n",
    "\"\"\"\n",
    "\n",
    "# get part of signal events\n",
    "print \"sig events quantity:\", len(xs_emu)\n",
    "print \"bkg events quantity:\", len(xb_emu)\n",
    "\n",
    "# get training data\n",
    "x_emu_train, x_emu_test, y_emu_train, y_emu_test, xs_emu_test, xb_emu_test = \\\n",
    "split_and_combine(xs_emu, xb_emu, shuffle_before_return = True)\n",
    "# get test data\n",
    "#x1, x_emu_test, y1, y_emu_test, xs_emu_test, xb_emu_test = \\\n",
    "#split_and_combine(xs_emu_part, xb_emu, shuffle_before_return = True)\n",
    "\n",
    "# select features wanted\n",
    "x_emu_train_selected = get_part_feature(x_emu_train, selected_features)\n",
    "x_emu_test_selected = get_part_feature(x_emu_test, selected_features)\n",
    "print \"train data shape:\", x_emu_train_selected.shape\n",
    "print \"test data shape:\", x_emu_test_selected.shape \n",
    "\n",
    "# train the model\n",
    "train_history = model_deep.get_model().fit(x_emu_train_selected, y_emu_train, batch_size = 100, epochs = 30,\n",
    "                validation_split = 0.25 , sample_weight = x_emu_train[:, -1], verbose = 1)\n",
    "score = model_deep.get_model().evaluate(x_emu_test_selected, y_emu_test, verbose=0, sample_weight = x_emu_test[:, -1])\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# load test (cross validation) data\n",
    "xs_emu_test_selected = get_part_feature(xs_emu, selected_features) # use full sample for validation\n",
    "xb_emu_test_selected = get_part_feature(xb_emu, selected_features) #\n",
    "\n",
    "# display scores\n",
    "plt.hist(xs_emu[:, 0], bins = 100, weights = xs_emu[:, -1], range = (0, 10000), histtype='step', label='signal', density=True)\n",
    "plt.xlabel(\"mass\")\n",
    "plt.ylabel(\"events\")\n",
    "plt.show()\n",
    "PlotScores(xs_emu_test_selected, xb_emu_test_selected, model_deep.get_model(), bins = 100, range = (0, 1), density = True)\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(train_history.history['acc'])\n",
    "plt.plot(train_history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim((0, 1))\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper center')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(train_history.history['loss'])\n",
    "plt.plot(train_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper center')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# make roc plots for signal\n",
    "print \"roc for sig and bkg\"\n",
    "plt.ylabel('tpr')\n",
    "plt.xlabel('fpr')\n",
    "predictions_dm = model_deep.get_model().predict(get_part_feature(x_emu_train, selected_features))\n",
    "fpr_dm, tpr_dm, threshold = roc_curve(y_emu_train, predictions_dm)\n",
    "plt.plot(fpr_dm, tpr_dm)\n",
    "predictions_dm = model_deep.get_model().predict(get_part_feature(x_emu_test, selected_features))\n",
    "fpr_dm_test, tpr_dm_test, threshold_test = roc_curve(y_emu_test, predictions_dm)\n",
    "plt.plot(fpr_dm_test, tpr_dm_test)\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print \"auc for train:\", auc(fpr_dm, tpr_dm)\n",
    "print \"auc for test: \", auc(fpr_dm_test, tpr_dm_test)\n",
    "\n",
    "print \"\\nDone.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* new ntuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [0, 1, 2, 3, 5, 6, 7, 15, 16, 17, 18, 19]\n",
    "\n",
    "# emu\n",
    "print \"Training emu.\"\n",
    "# get emu\n",
    "print \"Loading signal.\"\n",
    "xs_emu = modify_array(xs_dict_new['500GeV'], weight_id = -1, select_channel = True, channel_id = -4,\n",
    "                      norm = True, shuffle = True, shuffle_seed = int(time.time()))\n",
    "mass_sigma = get_mass_range(xs_emu[:, 0], xs_emu[:, -1])\n",
    "mass_min = 500 - mass_sigma\n",
    "mass_max = 500 + mass_sigma\n",
    "print \"mass range:\", mass_min, \"to\", mass_max\n",
    "print \"Loading background.\"\n",
    "xb_emu = modify_array(xb_dict_new['all'], weight_id = -1, remove_negative_weight = True, select_channel = True, channel_id = -4,\n",
    "                      select_mass = True, mass_id = 0, mass_min = mass_min, mass_max = mass_max,\n",
    "                      reset_mass = True, reset_mass_array = xs_emu, reset_mass_id = 0,\n",
    "                      norm = True, shuffle = True)\n",
    "# set model\n",
    "node_num = 300\n",
    "model_deep = Sequential()\n",
    "model_deep.add(Dense(node_num, kernel_initializer='uniform', input_dim = len(selected_features)))\n",
    "model_deep.add(BatchNormalization())\n",
    "model_deep.add(Dense(node_num, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "model_deep.add(BatchNormalization())\n",
    "model_deep.add(Dense(node_num, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "model_deep.add(BatchNormalization())\n",
    "model_deep.add(Dense(node_num, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "model_deep.add(BatchNormalization())\n",
    "model_deep.add(Dense(node_num, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "model_deep.add(BatchNormalization())\n",
    "model_deep.add(Dense(node_num, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "model_deep.add(BatchNormalization())\n",
    "model_deep.add(Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "# set loss, optimizer and evaluation metrics\n",
    "model_deep.compile(loss=\"binary_crossentropy\", optimizer=SGD(lr=0.025, decay=1e-6), metrics=[\"accuracy\"])\n",
    "\n",
    "# get part of signal events\n",
    "print \"sig events quantity:\", len(xs_emu)\n",
    "print \"bkg events quantity:\", len(xb_emu)\n",
    "\n",
    "# get training data\n",
    "x_emu_train, x_emu_test, y_emu_train, y_emu_test, xs_emu_test, xb_emu_test = \\\n",
    "split_and_combine(xs_emu, xb_emu, shuffle_before_return = True)\n",
    "# get test data\n",
    "#x1, x_emu_test, y1, y_emu_test, xs_emu_test, xb_emu_test = \\\n",
    "#split_and_combine(xs_emu_part, xb_emu, shuffle_before_return = True)\n",
    "\n",
    "# select features wanted\n",
    "x_emu_train_selected = get_part_feature(x_emu_train, selected_features)\n",
    "x_emu_test_selected = get_part_feature(x_emu_test, selected_features)\n",
    "print \"train data shape:\", x_emu_train_selected.shape\n",
    "print \"test data shape:\", x_emu_test_selected.shape \n",
    "\n",
    "# train the model\n",
    "train_history = model_deep.fit(x_emu_train_selected, y_emu_train, batch_size = 100, epochs = 30,\n",
    "                validation_split = 0.25 , sample_weight = x_emu_train[:, -1], verbose = 1)\n",
    "score = model_deep.evaluate(x_emu_test_selected, y_emu_test, verbose=0, sample_weight = x_emu_test[:, -1])\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# load test (cross validation) data\n",
    "xs_emu_test_selected = get_part_feature(xs_emu, selected_features) # use full sample for validation\n",
    "xb_emu_test_selected = get_part_feature(xb_emu, selected_features) #\n",
    "\n",
    "# display scores\n",
    "plt.hist(xs_emu[:, 0], bins = 100, weights = xs_emu[:, -1], range = (0, 10000), histtype='step', label='signal', density=True)\n",
    "plt.xlabel(\"mass\")\n",
    "plt.ylabel(\"events\")\n",
    "plt.show()\n",
    "PlotScores(xs_emu_test_selected, xb_emu_test_selected, model_deep, bins = 100, range = (0, 1), density = True)\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(train_history.history['acc'])\n",
    "plt.plot(train_history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim((0, 1))\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper center')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(train_history.history['loss'])\n",
    "plt.plot(train_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper center')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# make roc plots for signal\n",
    "print \"roc for sig and bkg\"\n",
    "plt.ylabel('tpr')\n",
    "plt.xlabel('fpr')\n",
    "predictions_dm = model_deep.predict(get_part_feature(x_emu_train, selected_features))\n",
    "fpr_dm, tpr_dm, threshold = roc_curve(y_emu_train, predictions_dm)\n",
    "plt.plot(fpr_dm, tpr_dm)\n",
    "predictions_dm = model_deep.predict(get_part_feature(x_emu_test, selected_features))\n",
    "fpr_dm_test, tpr_dm_test, threshold_test = roc_curve(y_emu_test, predictions_dm)\n",
    "plt.plot(fpr_dm_test, tpr_dm_test)\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print \"auc for train:\", auc(fpr_dm, tpr_dm)\n",
    "print \"auc for test: \", auc(fpr_dm_test, tpr_dm_test)\n",
    "\n",
    "print \"\\nDone.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## x - use 100% -> 1% signal for training\n",
    "number of background events fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage_list = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "percentage_list = [1]\n",
    "#percentage_list = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.5, 1]\n",
    "#percentage_list = [0.01, 0.1, 0.2, 0.5, 1]\n",
    "\n",
    "selected_features = [0, 1, 2, 3, 5, 6, 7, 15, 16, 17, 18, 19]\n",
    "\n",
    "# training with different percentage of signals\n",
    "for percent in percentage_list:\n",
    "    # set up training model\n",
    "    node_num = 300\n",
    "    model_deep = Sequential()\n",
    "    model_deep.add(Dense(node_num, kernel_initializer='uniform', input_dim = len(selected_features)))\n",
    "    model_deep.add(BatchNormalization())\n",
    "    model_deep.add(Dense(node_num, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "    model_deep.add(BatchNormalization())\n",
    "    model_deep.add(Dense(node_num, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "    model_deep.add(BatchNormalization())\n",
    "    #\"\"\"\n",
    "    model_deep.add(Dense(node_num, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "    model_deep.add(BatchNormalization())\n",
    "    model_deep.add(Dense(node_num, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "    model_deep.add(BatchNormalization())\n",
    "    model_deep.add(Dense(node_num, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "    model_deep.add(BatchNormalization())\n",
    "    #\"\"\"\n",
    "    model_deep.add(Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "    # set loss, optimizer and evaluation metrics\n",
    "    model_deep.compile(loss=\"binary_crossentropy\", optimizer=SGD(lr=0.01, decay=1e-6), metrics=[\"accuracy\"])\n",
    "\n",
    "    # get part of signal events\n",
    "    print \"*\" * 80\n",
    "    print \"use {}% signal for training\".format(percent * 100)\n",
    "    print \"sig events quantity:\", len(xs_emu) * percent\n",
    "    print \"bkg events quantity:\", len(xb_emu)\n",
    "    index_len = int(len(xs_emu) * percent)\n",
    "    index = np.random.choice(xs_emu.shape[0], index_len, replace = False)\n",
    "    xs_emu_part = xs_emu[index]\n",
    "    #print \"debug:\", xs_emu_part[0:5]\n",
    "    print \"debug:\", index[0:10]\n",
    "    print \"debug: \", xs_emu.shape[0], xs_emu_part.shape[0], index_len\n",
    "    \"\"\"\n",
    "    xs_emu_part, x2, y1, y2 = train_test_split(xs_emu, np.ones(len(xs_emu)), \n",
    "                                               test_size= 1 - percent, \n",
    "                                               random_state=3456, shuffle=True)\n",
    "    \"\"\"\n",
    "    xs_emu_part = modify_array(xs_emu_part, weight_id = -1, \n",
    "                               norm = True, shuffle = True)\n",
    "    xb_emu_temp = modify_array(xb_emu, weight_id = -1, mass_id = 0,\n",
    "                               reset_mass = True, reset_mass_array = xs_emu_part, reset_mass_id = 0,\n",
    "                               norm = True, shuffle = True)\n",
    "    \n",
    "    # get training data\n",
    "    x_emu_train, x_emu_test, y_emu_train, y_emu_test, xs_emu_test, xb_emu_test = \\\n",
    "    split_and_combine(xs_emu_part, xb_emu_temp, shuffle_before_return = True)\n",
    "    # get test data\n",
    "    #x1, x_emu_test, y1, y_emu_test, xs_emu_test, xb_emu_test = \\\n",
    "    #split_and_combine(xs_emu_part, xb_emu, shuffle_before_return = True)\n",
    "\n",
    "    # select features wanted\n",
    "    x_emu_train_selected = get_part_feature(x_emu_train, selected_features)\n",
    "    x_emu_test_selected = get_part_feature(x_emu_test, selected_features)\n",
    "    print \"train data shape:\", x_emu_train_selected.shape\n",
    "    print \"test data shape:\", x_emu_test_selected.shape \n",
    "\n",
    "    # train the model\n",
    "    train_history = model_deep.fit(x_emu_train_selected, y_emu_train, batch_size = 100, epochs = 30,\n",
    "                    validation_split = 0.25 , sample_weight = x_emu_train[:, -1], verbose = 1)\n",
    "    score = model_deep.evaluate(x_emu_test_selected, y_emu_test, verbose=0, sample_weight = x_emu_test[:, -1])\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    # load test (cross validation) data\n",
    "    xs_emu_test_selected = get_part_feature(xs_emu, selected_features) # use full sample for validation\n",
    "    xb_emu_test_selected = get_part_feature(xb_emu, selected_features) #\n",
    "    \n",
    "    # display scores\n",
    "    plt.hist(xs_emu[:, 0], bins = 100, weights = xs_emu[:, -1], range = (0, 10000), histtype='step', label='signal', density=True)\n",
    "    plt.xlabel(\"mass\")\n",
    "    plt.ylabel(\"events\")\n",
    "    plt.show()\n",
    "    PlotScores(xs_emu_test_selected, xb_emu_test_selected, model_deep, bins = 100, range = (0, 1), density = True)\n",
    "    \n",
    "    # summarize history for accuracy\n",
    "    plt.plot(train_history.history['acc'])\n",
    "    plt.plot(train_history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.ylim((0, 1))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper center')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    # summarize history for loss\n",
    "    plt.plot(train_history.history['loss'])\n",
    "    plt.plot(train_history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper center')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    # make roc plots for signal\n",
    "    print \"roc for sig and bkg\"\n",
    "    plt.ylabel('tpr')\n",
    "    plt.xlabel('fpr')\n",
    "    predictions_dm = model_deep.predict(get_part_feature(x_emu_train, selected_features))\n",
    "    fpr_dm, tpr_dm, threshold = roc_curve(y_emu_train, predictions_dm)\n",
    "    plt.plot(fpr_dm, tpr_dm)\n",
    "    predictions_dm = model_deep.predict(get_part_feature(x_emu_test, selected_features))\n",
    "    fpr_dm_test, tpr_dm_test, threshold_test = roc_curve(y_emu_test, predictions_dm)\n",
    "    plt.plot(fpr_dm_test, tpr_dm_test)\n",
    "    plt.legend(['train', 'test'], loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    print \"auc for train:\", auc(fpr_dm, tpr_dm)\n",
    "    print \"auc for test: \", auc(fpr_dm_test, tpr_dm_test)\n",
    "    \n",
    "    # clear model for next training\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {}\n",
    "a = 1\n",
    "b = 2\n",
    "test_dict['a'] = a\n",
    "test_dict['b'] = b\n",
    "print test_dict\n",
    "\n",
    "a = 10\n",
    "b = 20\n",
    "print test_dict\n",
    "\n",
    "test_dict['a'] = 100\n",
    "print test_dict\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../lfv_pdnn_code_v1/common/print_helper.py:26: UserWarning: test  warning\n",
      "  warnings.warn(content)\n"
     ]
    }
   ],
   "source": [
    "from lfv_pdnn_code_v1.common import print_helper\n",
    "\n",
    "print_helper.print_warning(\"test  warning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
